{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "BOT VIAJES 5 FOR Logic NLP- Creating a chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metaboll/Keras-FaceBook/blob/master/BOT_VIAJES_5_FOR_Logic_NLP_Creating_a_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjWkIC7XnQy",
        "colab_type": "text"
      },
      "source": [
        "# BOT VIAJES 5 + Train Sentence Generator + Deep Learning for NLP - Creating a chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45rrZi5QX0kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://towardsdatascience.com/deep-learning-for-nlp-creating-a-chatbot-with-keras-da5ca051e051\n",
        "#\n",
        "#\n",
        "#\n",
        "#CAMBIAMOS A LO GRANDE HACIENDO UN LOOP DE FRASES POR LO QUE SOLO VAMOS a ENTRENAR UNA FRASE \n",
        "\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "# GENERO LAS SENTENCIAS LOS ENTRENAMIENTOS Y LOS TEST\n",
        "# 26/8/2019 AHORA PONGO PREGUNTAS TRAMPAS PARA QUE CONTESTE IDONTKNOW\n",
        "# 29/8/2019 PONGO NOMBRES QUE NO CONOCE EN PREGUNTA WHERE Y AUNMENTO LONG FRASES \n",
        "# 29/8/2019 ADEMAS HAGO PREGUNTAS WHEN\n",
        "# 29/8/2019 TIPO DE TRANSPORTE\n",
        "# 04/9/2019 dia\n",
        "# 05/9/2019 dia. SOLO SE ANALIZA UNA ORACION. LA ORACION GRANDE SE DIVIDE EN PTOS\n",
        "# 07/9/2019 SE PONE SPACY EN ESPAÑOL\n",
        "# 09/9/2019. SE PONE O NO SUJETO Y SE METE PALABRAS DESCONOCIDAS COMO RUIDO.\n",
        "# FORMATEAR FECHAS\n",
        "# https://github.com/dateutil/dateutil\n",
        "# https://parzibyte.me/blog/2018/12/19/obtener-formatear-fecha-hora-actual-python/\n",
        "# https://stackoverflow.com/questions/19994396/best-way-to-identify-and-extract-dates-from-text-python\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYRNWD_-XnQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Library Imports\n",
        "import pickle\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJS7evoNX-5_",
        "colab_type": "code",
        "outputId": "7bf98cc1-4183-4f91-ec3b-414550de7121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeFbekUcquV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import secrets                              # imports secure module.\n",
        "import random\n",
        "\n",
        "secure_random = secrets.SystemRandom()      # creates a secure random object.\n",
        "def random_funct_generator(my_list, num_of_elements_to_select):\n",
        "  my_result = []\n",
        "  #list_of_random_items = secure_random.sample(group_of_items, num_to_select)\n",
        "  list_of_random_items = secure_random.sample(my_list, num_of_elements_to_select)\n",
        "  #first_random_item = list_of_random_items[0]\n",
        "  #second_random_item = list_of_random_items[1]\n",
        "  #print(first_random_item)\n",
        "  #print(second_random_item)\n",
        "  for n in range(num_of_elements_to_select):\n",
        "    my_result.append(list_of_random_items[n])\n",
        "  return my_result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXgU9AN4p1df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://stackoverflow.com/questions/2475518/python-how-to-append-elements-to-a-list-randomly\n",
        "from random import randrange, sample\n",
        "\n",
        "def random_insert(lst, item):\n",
        "    lst.insert(randrange(len(lst)+1), item)\n",
        "    #return\n",
        "\n",
        "array_of_noums = [\"yo\", \"mary\", \"eduard\", \"darlyn\",\"filip\",\"ferdinand\", \"sebastian\", \"margaret\", \"john\"]\n",
        "random_insert(array_of_noums , \"Jorge\")\n",
        "print(array_of_noums)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3yEwNv6sJBF",
        "colab_type": "code",
        "outputId": "ddb608f2-4a83-4893-ab70-fbd0dc02622a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def trainer_function2():\n",
        "  group_of_noums = (\"yo\", \"mary\", \"eduard\", \"darlyn\",\"filip\",\"ferdinand\", \"sebastian\", \"margaret\", \"john\") \n",
        "  group_of_verbs_places = (\"quiero_ir\", \"viajar\", \"desplazarse\", \"viaje_negocios\")\n",
        "  group_of_places = (\"venecia\",\"barcelona\", \"san_francisco\", \"new_york\",  \"amsterdam\",\"moscu\",\"londres\",\"el_cabo\")\n",
        "  \n",
        "  #group_of_verbs_time = (\"quiero_ir\", \"viajar_para\", \"seria\", \"la_intencion_es\",\"la_mejor_temporada\", \"intentaria\")\n",
        "  #group_of_time = (\"mes\",\"año\", \"proxima_temporada\", \"primavera\", \"verano\", \"dias\")\n",
        "  \n",
        "  group_of_verbs_medios = (\"quiero_ir\", \"ir\", \"queremos_viajar\", \"viajarian\", \"viajaria\", \"queremos_ir\" , \"mediante\", \"desplazarnos\",\"viajar\", \"viajamos\", \"la_intencion_es_viajar\",\"viajar_mediante\")\n",
        "  group_of_medios = (\"coche\",\"alquiler_coche\", \"autobus\", \"avion\", \"tren\", \"barco\",\"crucero\",\"ferry\",\"ave\",\"alvia\")\n",
        "  \n",
        "  group_of_noums_salida_date = ( \"yo\", \"nosotros\",\"ellos\",\"la_salida\")\n",
        "  group_of_verbo_salida_date = (\"quiero_ir\", \"saldriamos\", \"seria\",\"partiriamos\",\n",
        "                                \"ir\", \"queremos_viajar\", \"viajarian\", \"viajaria\",\n",
        "                                \"queremos_ir\" ,  \"viajamos\", \"la_intencion_es_viajar\",\"viajar\",\n",
        "                               \"queremos_salir\")\n",
        "  \n",
        "  group_of_noums_llegada_date = ( \"yo\", \"nosotros\",\"ellos\",\"el_regreso\")\n",
        "  group_of_verbo_llegada_date = (\"llegariamos\",\"la_llegada_seria\", \"desearimos_llegar\", \"tenemos_que_llegar\",\"tenemos_que_regresar\")\n",
        "  group_of_verbo_conditional_date = (\"sobre\", \"para\", \"el\", \"antes_del\", \"no_mas_tarde\")\n",
        "  \n",
        "  #group_of_persons\n",
        "\n",
        "\n",
        "  my_random_question = random.randint(0,1)\n",
        "  \n",
        "  l1 = random_funct_generator(group_of_noums, 1)\n",
        "  l2 = random_funct_generator(group_of_verbs_places, 1)\n",
        "  l3 = random_funct_generator(group_of_places, 1)\n",
        "  \n",
        "  #l4 = random_funct_generator(group_of_verbs_time, 1)\n",
        "  #l5 = random_funct_generator(group_of_time, 1)\n",
        "  \n",
        "  l6 = random_funct_generator(group_of_verbs_medios, 1)\n",
        "  l7 = random_funct_generator(group_of_medios, 1)\n",
        "  \n",
        "  l8 = random_funct_generator(group_of_noums_salida_date, 1)\n",
        "  l9 = random_funct_generator(group_of_verbo_salida_date, 1)\n",
        "  \n",
        "  l10 = random_funct_generator(group_of_noums_llegada_date, 1)\n",
        "  l11 = random_funct_generator(group_of_verbo_llegada_date, 1)\n",
        "  l12 = random_funct_generator(group_of_verbo_conditional_date, 1)\n",
        " \n",
        " \n",
        "  \n",
        "    \n",
        "  #Donde/Como/Salida/llegada\n",
        "  random_ = random.randint(0,3)\n",
        "  #\n",
        "  #   1 ORACIONES\n",
        "  #\n",
        "  ramdom_subjet  = random.randint(0,1) # sin sujeto o con sujeto\n",
        "  if random_ == 0:\n",
        "      random2 = random.randint(0,3)\n",
        "      if random2 == 0: \n",
        "        #my_phrase = [l1[0]  , l2[0] ,  \"a\"  ,  l3[0],  \".\", l6[0], \"en\", l7[0], \".\", l8[0], l9[0], \"el\", \"date\",\".\", l10[0], l11[0], l12[0], \"date2\",\".\"]\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l1[0]  , l2[0] ,  \"a\"  ,  l3[0],  \".\"]\n",
        "        else:\n",
        "          my_phrase = [ l2[0] ,  \"a\"  ,  l3[0],  \".\"]\n",
        "        my_question = [\"donde\" ,\"?\"] # [\"Donde\" ,  \"quiere\", l1[0]   ,\"ir\", \"?\"]\n",
        "        my_response = l3[0]\n",
        "        #print(\"1\")\n",
        "      elif random2 == 1:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l1[0]  , l6[0], \"en\", l7[0], \".\"]\n",
        "        else:\n",
        "          my_phrase = [ l6[0], \"en\", l7[0], \".\"]\n",
        "        my_question = [\"donde\" ,\"?\"] # [\"Donde\" ,  \"quiere\", l1[0]   ,\"ir\", \"?\"]\n",
        "        my_response = \"no_se_donde\"\n",
        "        #print(\"2\")\n",
        "      elif random2 == 2:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l8[0], l9[0], \"el\", \"date\",\".\"]\n",
        "        else:\n",
        "          my_phrase = [l9[0], \"el\", \"date\",\".\"]\n",
        "        my_question = [\"donde\" ,\"?\"] # [\"Donde\" ,  \"quiere\", l1[0]   ,\"ir\", \"?\"]\n",
        "        my_response = \"no_se_donde\"\n",
        "        #print(\"3\")\n",
        "      else:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [ l11[0], l12[0], \"date\",\".\"]\n",
        "        else:\n",
        "          my_phrase = [ l10[0], l11[0], l12[0], \"date\",\".\"]\n",
        "        \n",
        "        my_question = [\"donde\" ,\"?\"] # [\"Donde\" ,  \"quiere\", l1[0]   ,\"ir\", \"?\"]\n",
        "        my_response = \"no_se_donde\"\n",
        "        #print(\"4\")\n",
        "      \n",
        "  \n",
        "      \n",
        "  elif random_ == 1:\n",
        "      random2 = random.randint(0,3)\n",
        "      if random2 == 0:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l1[0]  , l6[0], \"en\", l7[0], \".\"]\n",
        "        else:\n",
        "          my_phrase = [ l6[0], \"en\", l7[0], \".\"]\n",
        "        my_question = [\"como\" ,   \"?\"] #[\"Como\" ,  \"quiere\", l1[0]   ,\"viajar\", \"?\"]\n",
        "        my_response = l7[0]\n",
        "        #print(\"5\")\n",
        "      elif random2 == 1:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l1[0]  , l2[0] ,  \"a\"  ,  l3[0],  \".\"]\n",
        "        else:\n",
        "          my_phrase = [ l2[0] ,  \"a\"  ,  l3[0],  \".\"]\n",
        "        my_question = [\"como\" ,   \"?\"] #[\"Como\" ,  \"quiere\", l1[0]   ,\"viajar\", \"?\"]\n",
        "        my_response = \"no_se_como\"\n",
        "        #print(\"6\")\n",
        "      elif random2 == 2:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l8[0], l9[0], \"el\", \"date\",\".\"]\n",
        "        else:\n",
        "          my_phrase = [l9[0], \"el\", \"date\",\".\"]\n",
        "        my_question = [\"como\" ,   \"?\"] #[\"Como\" ,  \"quiere\", l1[0]   ,\"viajar\", \"?\"]\n",
        "        my_response = \"no_se_como\"\n",
        "        #print(\"7\")\n",
        "      else:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [ l11[0], l12[0], \"date\",\".\"]\n",
        "        else:\n",
        "          my_phrase = [ l10[0], l11[0], l12[0], \"date\",\".\"]\n",
        "        my_question = [\"como\" ,   \"?\"] #[\"Como\" ,  \"quiere\", l1[0]   ,\"viajar\", \"?\"]\n",
        "        my_response = \"no_se_como\"\n",
        "        #print(\"8\")\n",
        "        \n",
        "  elif random_ == 2:\n",
        "      random2 = random.randint(0,3)\n",
        "      if random2 == 0:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l8[0], l9[0], \"el\", \"date\",\".\"]\n",
        "        else:\n",
        "          my_phrase = [l9[0], \"el\", \"date\",\".\"]\n",
        "        \n",
        "        my_question = [\"cuando\" ,\"salir\", \"?\"] #[\"Cuando\" ,  \"quiere\", l1[0]   ,\"salir\", \"?\"]\n",
        "        my_response = \"date\"\n",
        "        #print(\"9\")\n",
        "      elif random2 == 1:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l1[0]  , l2[0] ,  \"a\"  ,  l3[0],  \".\"]\n",
        "        else:\n",
        "          my_phrase = [ l2[0] ,  \"a\"  ,  l3[0],  \".\"]\n",
        "        my_question = [\"cuando\" ,\"salir\", \"?\"] #[\"Cuando\" ,  \"quiere\", l1[0]   ,\"salir\", \"?\"]\n",
        "        my_response = \"no_se_date\" \n",
        "        #print(\"10\")\n",
        "      elif random2 == 2:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l1[0]  , l6[0], \"en\", l7[0], \".\"]\n",
        "        else:\n",
        "          my_phrase = [ l6[0], \"en\", l7[0], \".\"]\n",
        "        my_question = [\"cuando\" ,\"salir\", \"?\"] #[\"Cuando\" ,  \"quiere\", l1[0]   ,\"salir\", \"?\"]\n",
        "        my_response = \"no_se_date\" \n",
        "        #print(\"11\")\n",
        "      else:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [ l11[0], l12[0], \"date\",\".\"]\n",
        "        else:\n",
        "          my_phrase = [ l10[0], l11[0], l12[0], \"date\",\".\"]\n",
        "        my_question = [\"cuando\" ,\"salir\", \"?\"] #[\"Cuando\" ,  \"quiere\", l1[0]   ,\"salir\", \"?\"]\n",
        "        my_response = \"no_se_date\" \n",
        "        #print(\"12\")\n",
        "        \n",
        "       \n",
        "  else:\n",
        "      random2 = random.randint(0,3)\n",
        "      if random2 == 0:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [ l11[0], l12[0], \"date2\",\".\"]\n",
        "        else:\n",
        "          my_phrase = [ l10[0], l11[0], l12[0], \"date2\",\".\"]\n",
        "        my_question = [\"cuando\"   ,\"llegar\", \"?\"] # [\"Cuando\" ,  \"quiere\", l1[0]   ,\"llegar\", \"?\"]\n",
        "        my_response = \"date2\"\n",
        "        #print(\"13\")\n",
        "      elif random2 == 1:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l1[0]  , l2[0] ,  \"a\"  ,  l3[0],  \".\"]\n",
        "        else:\n",
        "          my_phrase = [ l2[0] ,  \"a\"  ,  l3[0],  \".\"]\n",
        "        my_question = [\"cuando\"   ,\"llegar\", \"?\"] # [\"Cuando\" ,  \"quiere\", l1[0]   ,\"llegar\", \"?\"]\n",
        "        my_response = \"no_se_date2\"\n",
        "        #print(\"14\")\n",
        "      elif random2 == 2:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l1[0]  , l6[0], \"en\", l7[0], \".\"]\n",
        "        else:\n",
        "          my_phrase = [ l6[0], \"en\", l7[0], \".\"]\n",
        "        \n",
        "        my_question = [\"cuando\"   ,\"llegar\", \"?\"] # [\"Cuando\" ,  \"quiere\", l1[0]   ,\"llegar\", \"?\"]\n",
        "        my_response = \"no_se_date2\"\n",
        "        #print(\"15\")\n",
        "      else:\n",
        "        if ramdom_subjet == 0:\n",
        "          my_phrase = [l8[0], l9[0], \"el\", \"date\",\".\"]\n",
        "        else:\n",
        "          my_phrase = [l9[0], \"el\", \"date\",\".\"]\n",
        "        my_question = [\"cuando\"   ,\"llegar\", \"?\"] # [\"Cuando\" ,  \"quiere\", l1[0]   ,\"llegar\", \"?\"]\n",
        "        my_response = \"no_se_date2\"\n",
        "        #print(\"16\")\n",
        "        \n",
        "  \n",
        "        \n",
        "        \n",
        "#my_response = \"no_se_como\"\n",
        "#my_response = \"no_se_donde\"\n",
        "#my_response = \"no_se_date\"  \n",
        "#my_response = \"no_se_date2\" \n",
        "\n",
        "\n",
        "  \n",
        "  return(my_phrase ,my_question,my_response)\n",
        "\n",
        "#print(trainer_function())\n",
        "\n",
        "def all_data_trainer_function2(number_of_elem):\n",
        "  solution = []\n",
        "  for n in range(number_of_elem):\n",
        "    solution.append(trainer_function2())\n",
        "  return solution\n",
        "    \n",
        "    \n",
        "#print(all_data_trainer_function(2))\n",
        "train_data = all_data_trainer_function2(10000)\n",
        "print(\"1st phase.\")\n",
        "test_data = all_data_trainer_function2(1000)\n",
        "print(\"2 nd phase.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1st phase.\n",
            "2 nd phase.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaNgnh70YMP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "#with open('intents.json') as json_data:\n",
        "with open('drive/My Drive/data/data/train_qa.txt', 'rb') as f:\n",
        "    #intents = json.load(json_data)\n",
        "    train_data = pickle.load(f)\n",
        "with open('drive/My Drive/data/data/test_qa.txt', 'rb') as f:\n",
        "    test_data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nc4qU4bXnRm",
        "colab_type": "code",
        "outputId": "0ed3d3e2-9689-4fd9-a0fa-c1345bda5a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Number of training instances\n",
        "len(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDNi6p7KXnR6",
        "colab_type": "code",
        "outputId": "1a3dfed4-b548-4b91-c825-d3d76bd4cb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Number of test instances\n",
        "len(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7V59hmzXnSS",
        "colab_type": "code",
        "outputId": "0e723231-fefa-4f19-e873-1809593a6469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Example of one of the instances\n",
        "#train_data[10]\n",
        "print(train_data[17])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['Sandra', 'moved', 'to', 'the', 'office', '.', 'John', 'went', 'back', 'to', 'the', 'garden', '.', 'Sandra', 'went', 'to', 'the', 'hallway', '.', 'Sandra', 'went', 'to', 'the', 'kitchen', '.', 'Mary', 'went', 'to', 'the', 'office', '.', 'Sandra', 'got', 'the', 'apple', 'there', '.'], ['Is', 'Sandra', 'in', 'the', 'kitchen', '?'], 'yes')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MpomqHlXnSl",
        "colab_type": "code",
        "outputId": "a0768001-9467-4cb1-8347-273c272a4485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "' '.join(train_data[10][0])\n",
        "n = 0\n",
        "while True:\n",
        "    my_s = ' '.join(train_data[n][0])\n",
        "    my_s1 = ' '.join(train_data[n][1])\n",
        "    my_s2 = ''.join(train_data[n][2])\n",
        "    print(my_s,\"/\",my_s1,\"/\",my_s2)\n",
        "    #print(my_s1)\n",
        "    \n",
        "    #print(my_s2)\n",
        "    #time.sleep(.5)\n",
        "    n+= 1\n",
        "    if n == 100:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mary moved to the bathroom . Sandra journeyed to the bedroom . / Is Sandra in the hallway ? / no\n",
            "Mary moved to the bathroom . Sandra journeyed to the bedroom . Mary went back to the bedroom . Daniel went back to the hallway . / Is Daniel in the bathroom ? / no\n",
            "Mary moved to the bathroom . Sandra journeyed to the bedroom . Mary went back to the bedroom . Daniel went back to the hallway . Sandra went to the kitchen . Daniel went back to the bathroom . / Is Daniel in the office ? / no\n",
            "Mary moved to the bathroom . Sandra journeyed to the bedroom . Mary went back to the bedroom . Daniel went back to the hallway . Sandra went to the kitchen . Daniel went back to the bathroom . Daniel picked up the football there . Daniel went to the bedroom . / Is Daniel in the bedroom ? / yes\n",
            "Mary moved to the bathroom . Sandra journeyed to the bedroom . Mary went back to the bedroom . Daniel went back to the hallway . Sandra went to the kitchen . Daniel went back to the bathroom . Daniel picked up the football there . Daniel went to the bedroom . John travelled to the office . Sandra went to the garden . / Is Daniel in the bedroom ? / yes\n",
            "Sandra got the football there . Mary went to the bedroom . / Is Mary in the bedroom ? / yes\n",
            "Sandra got the football there . Mary went to the bedroom . Daniel got the apple there . Sandra travelled to the hallway . / Is Sandra in the office ? / no\n",
            "Sandra got the football there . Mary went to the bedroom . Daniel got the apple there . Sandra travelled to the hallway . Sandra moved to the garden . Mary travelled to the kitchen . / Is Sandra in the bathroom ? / no\n",
            "Sandra got the football there . Mary went to the bedroom . Daniel got the apple there . Sandra travelled to the hallway . Sandra moved to the garden . Mary travelled to the kitchen . Sandra went back to the bedroom . Daniel put down the apple . / Is Sandra in the bathroom ? / no\n",
            "Sandra got the football there . Mary went to the bedroom . Daniel got the apple there . Sandra travelled to the hallway . Sandra moved to the garden . Mary travelled to the kitchen . Sandra went back to the bedroom . Daniel put down the apple . Sandra put down the football . Sandra journeyed to the office . / Is Mary in the kitchen ? / yes\n",
            "Sandra went back to the hallway . Sandra moved to the office . / Is Sandra in the office ? / yes\n",
            "Sandra went back to the hallway . Sandra moved to the office . Mary moved to the hallway . Daniel journeyed to the garden . / Is Mary in the hallway ? / yes\n",
            "Sandra went back to the hallway . Sandra moved to the office . Mary moved to the hallway . Daniel journeyed to the garden . Mary journeyed to the office . Mary went back to the hallway . / Is Mary in the hallway ? / yes\n",
            "Sandra went back to the hallway . Sandra moved to the office . Mary moved to the hallway . Daniel journeyed to the garden . Mary journeyed to the office . Mary went back to the hallway . John grabbed the apple there . Mary moved to the garden . / Is Mary in the hallway ? / no\n",
            "Sandra went back to the hallway . Sandra moved to the office . Mary moved to the hallway . Daniel journeyed to the garden . Mary journeyed to the office . Mary went back to the hallway . John grabbed the apple there . Mary moved to the garden . Daniel went back to the hallway . John journeyed to the bathroom . / Is Mary in the garden ? / yes\n",
            "Sandra moved to the office . John went back to the garden . / Is Sandra in the office ? / yes\n",
            "Sandra moved to the office . John went back to the garden . Sandra went to the hallway . Sandra went to the kitchen . / Is Sandra in the bathroom ? / no\n",
            "Sandra moved to the office . John went back to the garden . Sandra went to the hallway . Sandra went to the kitchen . Mary went to the office . Sandra got the apple there . / Is Sandra in the kitchen ? / yes\n",
            "Sandra moved to the office . John went back to the garden . Sandra went to the hallway . Sandra went to the kitchen . Mary went to the office . Sandra got the apple there . Mary journeyed to the hallway . Mary journeyed to the bedroom . / Is Mary in the bedroom ? / yes\n",
            "Sandra moved to the office . John went back to the garden . Sandra went to the hallway . Sandra went to the kitchen . Mary went to the office . Sandra got the apple there . Mary journeyed to the hallway . Mary journeyed to the bedroom . Mary journeyed to the garden . Mary went to the kitchen . / Is Mary in the kitchen ? / yes\n",
            "Daniel got the apple there . John picked up the football there . Daniel left the apple . Daniel moved to the kitchen . / Is Daniel in the bedroom ? / no\n",
            "Daniel got the apple there . John picked up the football there . Daniel left the apple . Daniel moved to the kitchen . Sandra went back to the bathroom . Sandra grabbed the apple there . / Is Sandra in the bathroom ? / yes\n",
            "Daniel got the apple there . John picked up the football there . Daniel left the apple . Daniel moved to the kitchen . Sandra went back to the bathroom . Sandra grabbed the apple there . John dropped the football . John got the football there . / Is Sandra in the bedroom ? / no\n",
            "Daniel got the apple there . John picked up the football there . Daniel left the apple . Daniel moved to the kitchen . Sandra went back to the bathroom . Sandra grabbed the apple there . John dropped the football . John got the football there . Sandra put down the apple . Sandra grabbed the apple there . Daniel travelled to the hallway . Sandra left the apple . / Is Daniel in the office ? / no\n",
            "Daniel got the apple there . John picked up the football there . Daniel left the apple . Daniel moved to the kitchen . Sandra went back to the bathroom . Sandra grabbed the apple there . John dropped the football . John got the football there . Sandra put down the apple . Sandra grabbed the apple there . Daniel travelled to the hallway . Sandra left the apple . John dropped the football . Sandra picked up the apple there . / Is Daniel in the kitchen ? / no\n",
            "Sandra journeyed to the garden . Sandra went back to the bedroom . / Is Sandra in the bathroom ? / no\n",
            "Sandra journeyed to the garden . Sandra went back to the bedroom . Daniel took the apple there . Sandra travelled to the office . / Is Sandra in the office ? / yes\n",
            "Sandra journeyed to the garden . Sandra went back to the bedroom . Daniel took the apple there . Sandra travelled to the office . John went to the hallway . Sandra moved to the kitchen . / Is John in the office ? / no\n",
            "Sandra journeyed to the garden . Sandra went back to the bedroom . Daniel took the apple there . Sandra travelled to the office . John went to the hallway . Sandra moved to the kitchen . Daniel journeyed to the bathroom . Daniel went back to the bedroom . / Is Sandra in the office ? / no\n",
            "Sandra journeyed to the garden . Sandra went back to the bedroom . Daniel took the apple there . Sandra travelled to the office . John went to the hallway . Sandra moved to the kitchen . Daniel journeyed to the bathroom . Daniel went back to the bedroom . Daniel travelled to the kitchen . Sandra went to the bedroom . / Is Sandra in the hallway ? / no\n",
            "John went to the bathroom . Sandra took the football there . / Is John in the bathroom ? / yes\n",
            "John went to the bathroom . Sandra took the football there . Mary journeyed to the kitchen . John journeyed to the bedroom . / Is John in the bedroom ? / yes\n",
            "John went to the bathroom . Sandra took the football there . Mary journeyed to the kitchen . John journeyed to the bedroom . John took the apple there . John left the apple . / Is Mary in the hallway ? / no\n",
            "John went to the bathroom . Sandra took the football there . Mary journeyed to the kitchen . John journeyed to the bedroom . John took the apple there . John left the apple . Daniel grabbed the milk there . Sandra dropped the football . / Is John in the bedroom ? / yes\n",
            "John went to the bathroom . Sandra took the football there . Mary journeyed to the kitchen . John journeyed to the bedroom . John took the apple there . John left the apple . Daniel grabbed the milk there . Sandra dropped the football . Mary picked up the football there . John got the apple there . Mary dropped the football . Daniel went back to the kitchen . / Is Daniel in the bathroom ? / no\n",
            "Sandra went to the garden . Sandra grabbed the milk there . / Is Sandra in the hallway ? / no\n",
            "Sandra went to the garden . Sandra grabbed the milk there . Mary moved to the office . Mary went to the garden . / Is Mary in the kitchen ? / no\n",
            "Sandra went to the garden . Sandra grabbed the milk there . Mary moved to the office . Mary went to the garden . Daniel went back to the office . Mary journeyed to the bedroom . / Is Mary in the bathroom ? / no\n",
            "Sandra went to the garden . Sandra grabbed the milk there . Mary moved to the office . Mary went to the garden . Daniel went back to the office . Mary journeyed to the bedroom . Sandra went back to the hallway . Sandra journeyed to the office . / Is Sandra in the office ? / yes\n",
            "Sandra went to the garden . Sandra grabbed the milk there . Mary moved to the office . Mary went to the garden . Daniel went back to the office . Mary journeyed to the bedroom . Sandra went back to the hallway . Sandra journeyed to the office . Sandra journeyed to the garden . Mary journeyed to the hallway . / Is Mary in the bathroom ? / no\n",
            "Sandra grabbed the football there . Sandra went back to the hallway . / Is Sandra in the garden ? / no\n",
            "Sandra grabbed the football there . Sandra went back to the hallway . Mary journeyed to the kitchen . Mary moved to the office . / Is Mary in the office ? / yes\n",
            "Sandra grabbed the football there . Sandra went back to the hallway . Mary journeyed to the kitchen . Mary moved to the office . John moved to the kitchen . Sandra put down the football there . / Is John in the bathroom ? / no\n",
            "Sandra grabbed the football there . Sandra went back to the hallway . Mary journeyed to the kitchen . Mary moved to the office . John moved to the kitchen . Sandra put down the football there . Mary took the apple there . John travelled to the hallway . / Is John in the garden ? / no\n",
            "Sandra grabbed the football there . Sandra went back to the hallway . Mary journeyed to the kitchen . Mary moved to the office . John moved to the kitchen . Sandra put down the football there . Mary took the apple there . John travelled to the hallway . Daniel moved to the hallway . Mary travelled to the kitchen . / Is John in the garden ? / no\n",
            "Daniel journeyed to the bedroom . John moved to the bedroom . / Is Daniel in the hallway ? / no\n",
            "Daniel journeyed to the bedroom . John moved to the bedroom . Daniel took the apple there . Mary moved to the office . / Is Mary in the office ? / yes\n",
            "Daniel journeyed to the bedroom . John moved to the bedroom . Daniel took the apple there . Mary moved to the office . Sandra went to the office . Daniel went to the bathroom . / Is Daniel in the bathroom ? / yes\n",
            "Daniel journeyed to the bedroom . John moved to the bedroom . Daniel took the apple there . Mary moved to the office . Sandra went to the office . Daniel went to the bathroom . Daniel put down the apple . Sandra journeyed to the kitchen . / Is Mary in the hallway ? / no\n",
            "Daniel journeyed to the bedroom . John moved to the bedroom . Daniel took the apple there . Mary moved to the office . Sandra went to the office . Daniel went to the bathroom . Daniel put down the apple . Sandra journeyed to the kitchen . Daniel grabbed the apple there . Daniel took the milk there . / Is Daniel in the kitchen ? / no\n",
            "Mary went back to the bathroom . Daniel went to the office . / Is Mary in the bathroom ? / yes\n",
            "Mary went back to the bathroom . Daniel went to the office . Mary got the milk there . Sandra travelled to the bathroom . / Is Sandra in the bathroom ? / yes\n",
            "Mary went back to the bathroom . Daniel went to the office . Mary got the milk there . Sandra travelled to the bathroom . Mary went back to the bedroom . Mary picked up the apple there . / Is Sandra in the bathroom ? / yes\n",
            "Mary went back to the bathroom . Daniel went to the office . Mary got the milk there . Sandra travelled to the bathroom . Mary went back to the bedroom . Mary picked up the apple there . Sandra moved to the hallway . Mary discarded the apple . / Is Mary in the bedroom ? / yes\n",
            "Mary went back to the bathroom . Daniel went to the office . Mary got the milk there . Sandra travelled to the bathroom . Mary went back to the bedroom . Mary picked up the apple there . Sandra moved to the hallway . Mary discarded the apple . Daniel journeyed to the kitchen . Mary took the apple there . / Is Daniel in the bathroom ? / no\n",
            "Daniel journeyed to the hallway . Mary went back to the kitchen . / Is Mary in the kitchen ? / yes\n",
            "Daniel journeyed to the hallway . Mary went back to the kitchen . John moved to the hallway . John picked up the apple there . / Is Daniel in the kitchen ? / no\n",
            "Daniel journeyed to the hallway . Mary went back to the kitchen . John moved to the hallway . John picked up the apple there . Daniel moved to the bathroom . John went to the garden . / Is Mary in the kitchen ? / yes\n",
            "Daniel journeyed to the hallway . Mary went back to the kitchen . John moved to the hallway . John picked up the apple there . Daniel moved to the bathroom . John went to the garden . Mary moved to the hallway . John dropped the apple . / Is John in the bedroom ? / no\n",
            "Daniel journeyed to the hallway . Mary went back to the kitchen . John moved to the hallway . John picked up the apple there . Daniel moved to the bathroom . John went to the garden . Mary moved to the hallway . John dropped the apple . Sandra grabbed the apple there . Sandra left the apple . / Is John in the garden ? / yes\n",
            "John journeyed to the hallway . Sandra took the football there . / Is John in the bathroom ? / no\n",
            "John journeyed to the hallway . Sandra took the football there . John journeyed to the bathroom . John journeyed to the office . / Is John in the garden ? / no\n",
            "John journeyed to the hallway . Sandra took the football there . John journeyed to the bathroom . John journeyed to the office . Daniel journeyed to the hallway . Sandra discarded the football there . / Is John in the garden ? / no\n",
            "John journeyed to the hallway . Sandra took the football there . John journeyed to the bathroom . John journeyed to the office . Daniel journeyed to the hallway . Sandra discarded the football there . Daniel moved to the office . John picked up the apple there . / Is Daniel in the kitchen ? / no\n",
            "John journeyed to the hallway . Sandra took the football there . John journeyed to the bathroom . John journeyed to the office . Daniel journeyed to the hallway . Sandra discarded the football there . Daniel moved to the office . John picked up the apple there . John moved to the bathroom . Mary journeyed to the bathroom . / Is Mary in the bathroom ? / yes\n",
            "Sandra went back to the bathroom . Sandra took the apple there . / Is Sandra in the bathroom ? / yes\n",
            "Sandra went back to the bathroom . Sandra took the apple there . Sandra picked up the football there . John journeyed to the bathroom . / Is John in the bathroom ? / yes\n",
            "Sandra went back to the bathroom . Sandra took the apple there . Sandra picked up the football there . John journeyed to the bathroom . Mary went back to the bedroom . John moved to the kitchen . / Is Mary in the kitchen ? / no\n",
            "Sandra went back to the bathroom . Sandra took the apple there . Sandra picked up the football there . John journeyed to the bathroom . Mary went back to the bedroom . John moved to the kitchen . John went to the office . Sandra took the milk there . / Is Mary in the bedroom ? / yes\n",
            "Sandra went back to the bathroom . Sandra took the apple there . Sandra picked up the football there . John journeyed to the bathroom . Mary went back to the bedroom . John moved to the kitchen . John went to the office . Sandra took the milk there . Sandra left the football . Sandra left the apple there . / Is John in the garden ? / no\n",
            "John went back to the hallway . Daniel travelled to the hallway . / Is Daniel in the bathroom ? / no\n",
            "John went back to the hallway . Daniel travelled to the hallway . Daniel picked up the milk there . Daniel left the milk . / Is John in the hallway ? / yes\n",
            "John went back to the hallway . Daniel travelled to the hallway . Daniel picked up the milk there . Daniel left the milk . Daniel went to the bedroom . Daniel went back to the hallway . / Is Daniel in the hallway ? / yes\n",
            "John went back to the hallway . Daniel travelled to the hallway . Daniel picked up the milk there . Daniel left the milk . Daniel went to the bedroom . Daniel went back to the hallway . Daniel picked up the milk there . John picked up the football there . / Is Daniel in the hallway ? / yes\n",
            "John went back to the hallway . Daniel travelled to the hallway . Daniel picked up the milk there . Daniel left the milk . Daniel went to the bedroom . Daniel went back to the hallway . Daniel picked up the milk there . John picked up the football there . Mary went back to the bathroom . Daniel moved to the bedroom . / Is Daniel in the hallway ? / no\n",
            "Mary got the football there . Daniel travelled to the bedroom . / Is Daniel in the hallway ? / no\n",
            "Mary got the football there . Daniel travelled to the bedroom . Sandra got the milk there . Sandra put down the milk . / Is Daniel in the garden ? / no\n",
            "Mary got the football there . Daniel travelled to the bedroom . Sandra got the milk there . Sandra put down the milk . Sandra journeyed to the office . Daniel went back to the kitchen . / Is Sandra in the bedroom ? / no\n",
            "Mary got the football there . Daniel travelled to the bedroom . Sandra got the milk there . Sandra put down the milk . Sandra journeyed to the office . Daniel went back to the kitchen . Daniel moved to the bathroom . Mary left the football . / Is Daniel in the office ? / no\n",
            "Mary got the football there . Daniel travelled to the bedroom . Sandra got the milk there . Sandra put down the milk . Sandra journeyed to the office . Daniel went back to the kitchen . Daniel moved to the bathroom . Mary left the football . Mary went back to the kitchen . John journeyed to the kitchen . / Is Daniel in the bedroom ? / no\n",
            "Sandra went back to the office . Mary journeyed to the bedroom . / Is Mary in the bedroom ? / yes\n",
            "Sandra went back to the office . Mary journeyed to the bedroom . Sandra got the football there . Sandra put down the football . / Is Sandra in the office ? / yes\n",
            "Sandra went back to the office . Mary journeyed to the bedroom . Sandra got the football there . Sandra put down the football . John went back to the office . Mary grabbed the milk there . / Is Mary in the bedroom ? / yes\n",
            "Sandra went back to the office . Mary journeyed to the bedroom . Sandra got the football there . Sandra put down the football . John went back to the office . Mary grabbed the milk there . Sandra picked up the football there . John went to the bedroom . / Is John in the bedroom ? / yes\n",
            "Sandra went back to the office . Mary journeyed to the bedroom . Sandra got the football there . Sandra put down the football . John went back to the office . Mary grabbed the milk there . Sandra picked up the football there . John went to the bedroom . Sandra went to the kitchen . Daniel went back to the bathroom . / Is Sandra in the office ? / no\n",
            "Sandra went to the kitchen . Sandra travelled to the bathroom . / Is Sandra in the bathroom ? / yes\n",
            "Sandra went to the kitchen . Sandra travelled to the bathroom . Daniel went back to the bedroom . Mary journeyed to the garden . / Is Sandra in the bathroom ? / yes\n",
            "Sandra went to the kitchen . Sandra travelled to the bathroom . Daniel went back to the bedroom . Mary journeyed to the garden . Mary went back to the office . Daniel went back to the bathroom . / Is Daniel in the hallway ? / no\n",
            "Sandra went to the kitchen . Sandra travelled to the bathroom . Daniel went back to the bedroom . Mary journeyed to the garden . Mary went back to the office . Daniel went back to the bathroom . John went back to the bedroom . Mary went back to the garden . / Is Daniel in the bathroom ? / yes\n",
            "Sandra went to the kitchen . Sandra travelled to the bathroom . Daniel went back to the bedroom . Mary journeyed to the garden . Mary went back to the office . Daniel went back to the bathroom . John went back to the bedroom . Mary went back to the garden . Daniel went to the bedroom . Mary moved to the bathroom . / Is Daniel in the office ? / no\n",
            "John travelled to the garden . Sandra travelled to the garden . / Is John in the bedroom ? / no\n",
            "John travelled to the garden . Sandra travelled to the garden . Mary moved to the bedroom . Mary travelled to the kitchen . / Is John in the garden ? / yes\n",
            "John travelled to the garden . Sandra travelled to the garden . Mary moved to the bedroom . Mary travelled to the kitchen . John went to the office . John grabbed the milk there . / Is Sandra in the bedroom ? / no\n",
            "John travelled to the garden . Sandra travelled to the garden . Mary moved to the bedroom . Mary travelled to the kitchen . John went to the office . John grabbed the milk there . Daniel journeyed to the hallway . Sandra got the football there . / Is Daniel in the kitchen ? / no\n",
            "John travelled to the garden . Sandra travelled to the garden . Mary moved to the bedroom . Mary travelled to the kitchen . John went to the office . John grabbed the milk there . Daniel journeyed to the hallway . Sandra got the football there . Sandra journeyed to the bedroom . Sandra put down the football . / Is Daniel in the hallway ? / yes\n",
            "Daniel grabbed the apple there . Daniel went to the bedroom . / Is Daniel in the bedroom ? / yes\n",
            "Daniel grabbed the apple there . Daniel went to the bedroom . John moved to the garden . Sandra journeyed to the office . / Is John in the bathroom ? / no\n",
            "Daniel grabbed the apple there . Daniel went to the bedroom . John moved to the garden . Sandra journeyed to the office . Daniel put down the apple . Mary went to the bedroom . / Is Daniel in the bedroom ? / yes\n",
            "Daniel grabbed the apple there . Daniel went to the bedroom . John moved to the garden . Sandra journeyed to the office . Daniel put down the apple . Mary went to the bedroom . Mary grabbed the apple there . Sandra went back to the garden . / Is Mary in the bedroom ? / yes\n",
            "Daniel grabbed the apple there . Daniel went to the bedroom . John moved to the garden . Sandra journeyed to the office . Daniel put down the apple . Mary went to the bedroom . Mary grabbed the apple there . Sandra went back to the garden . Mary went to the kitchen . Daniel went to the office . / Is Mary in the garden ? / no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxN5h3AfXnSy",
        "colab_type": "code",
        "outputId": "a51a9dcd-27e0-4a75-e233-b9a1a606e0df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "' '.join(train_data[10][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Is Sandra in the office ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z7J7orCXnTK",
        "colab_type": "code",
        "outputId": "df33cb66-2199-49a6-8fcf-19621646e671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data[10][2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQtieDAKXnTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#First we need to create a vocabulary with our data\n",
        "#For this we will use the training data only to - On the video it uses both\n",
        "#train and test \n",
        "#Might have to use training and test later, as the dataset has very\n",
        "#few words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63tg4yMtXnTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#First we will build a set of all the words in the dataset:\n",
        "vocab = set()\n",
        "for story, question, answer in train_data:\n",
        "    vocab = vocab.union(set(story)) #Set returns unique words in the sentence\n",
        "                                    #Union returns the unique common elements from a two sets\n",
        "    vocab = vocab.union(set(question))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJXVx_YVXnT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.add('no')\n",
        "vocab.add('si')\n",
        "vocab.add('no_ne')\n",
        "vocab.add('cuando')\n",
        "vocab.add('donde')\n",
        "vocab.add('como')\n",
        "vocab.add('viajar')\n",
        "vocab.add('londres')\n",
        "vocab.add('date')\n",
        "vocab.add('date2')\n",
        "vocab.add('no_se_date')\n",
        "vocab.add('no_se_date2')\n",
        "vocab.add('no_se_como')\n",
        "vocab.add('no_se_donde')\n",
        "vocab.add('linda')#----> un nombre con el que no ha sido entrenado..... a ver que pasa......"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lNdrc-E7XnUV",
        "colab_type": "code",
        "outputId": "447368be-ee18-4dd5-9fec-5a79d524d149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.',\n",
              " '?',\n",
              " 'Daniel',\n",
              " 'Is',\n",
              " 'John',\n",
              " 'Mary',\n",
              " 'Sandra',\n",
              " 'apple',\n",
              " 'back',\n",
              " 'bathroom',\n",
              " 'bedroom',\n",
              " 'como',\n",
              " 'cuando',\n",
              " 'date',\n",
              " 'date2',\n",
              " 'discarded',\n",
              " 'donde',\n",
              " 'down',\n",
              " 'dropped',\n",
              " 'football',\n",
              " 'garden',\n",
              " 'got',\n",
              " 'grabbed',\n",
              " 'hallway',\n",
              " 'in',\n",
              " 'journeyed',\n",
              " 'kitchen',\n",
              " 'left',\n",
              " 'linda',\n",
              " 'londres',\n",
              " 'milk',\n",
              " 'moved',\n",
              " 'no',\n",
              " 'no_ne',\n",
              " 'no_se_como',\n",
              " 'no_se_date',\n",
              " 'no_se_date2',\n",
              " 'no_se_donde',\n",
              " 'office',\n",
              " 'picked',\n",
              " 'put',\n",
              " 'si',\n",
              " 'the',\n",
              " 'there',\n",
              " 'to',\n",
              " 'took',\n",
              " 'travelled',\n",
              " 'up',\n",
              " 'viajar',\n",
              " 'went'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g90OOs00XnUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate len and add 1 for Keras placeholder - Placeholders are used to feed in the data to the network. \n",
        "#They need a data type, and have optional shape arguements.\n",
        "#They will be empty at first, and then the data will get fed into the placeholder\n",
        "vocab_len = len(vocab) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqKC9IVdXnU3",
        "colab_type": "code",
        "outputId": "b25aef24-4d39-4176-d6fb-0d72069eef1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_len"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR3WNCNQXnVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we are going to calculate the longest story and the longest question\n",
        "#We need this for the Keras pad sequences. \n",
        "#Keras training layers expect all of the input to have the same length, so \n",
        "#we need to pad \n",
        "all_data = test_data + train_data\n",
        "all_story_lens = [len(data[0]) for data in all_data]\n",
        "max_story_len = (max(all_story_lens))\n",
        "max_question_len = max([len(data[1]) for data in all_data])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZWK4HRhXnV2",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCB-NSzUXnV5",
        "colab_type": "text"
      },
      "source": [
        "First, we will go through a manual process of how to vectorize the data, and then we will create a function that does this automatically for us. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUedKk9vXnV8",
        "colab_type": "code",
        "outputId": "0a133880-b1ae-46ed-ddd8-5d8806fd52bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3w3EWQBXnWE",
        "colab_type": "code",
        "outputId": "505d2c4e-92cb-4421-9082-e1b33f85c86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "#Tokenize the stories, questions and answers:\n",
        "train_story_text = []\n",
        "train_question_text = []\n",
        "train_answers = []\n",
        "#Create an instance of the tokenizer object:\n",
        "tokenizer = Tokenizer(filters = [])\n",
        "tokenizer.fit_on_texts(vocab)\n",
        "#Dictionary that maps every word in our vocab to an index\n",
        "# It has been automatically lowercased\n",
        "#This tokenizer can give different indexes for different words depending on when we run it\n",
        "tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 9,\n",
              " '?': 11,\n",
              " 'apple': 43,\n",
              " 'back': 45,\n",
              " 'bathroom': 33,\n",
              " 'bedroom': 47,\n",
              " 'como': 31,\n",
              " 'cuando': 24,\n",
              " 'daniel': 14,\n",
              " 'date': 36,\n",
              " 'date2': 23,\n",
              " 'discarded': 37,\n",
              " 'donde': 22,\n",
              " 'down': 21,\n",
              " 'dropped': 18,\n",
              " 'football': 41,\n",
              " 'garden': 50,\n",
              " 'got': 40,\n",
              " 'grabbed': 38,\n",
              " 'hallway': 2,\n",
              " 'in': 26,\n",
              " 'is': 6,\n",
              " 'john': 8,\n",
              " 'journeyed': 42,\n",
              " 'kitchen': 10,\n",
              " 'left': 39,\n",
              " 'linda': 13,\n",
              " 'londres': 29,\n",
              " 'mary': 4,\n",
              " 'milk': 49,\n",
              " 'moved': 34,\n",
              " 'no': 44,\n",
              " 'no_ne': 30,\n",
              " 'no_se_como': 12,\n",
              " 'no_se_date': 19,\n",
              " 'no_se_date2': 1,\n",
              " 'no_se_donde': 32,\n",
              " 'office': 27,\n",
              " 'picked': 16,\n",
              " 'put': 25,\n",
              " 'sandra': 7,\n",
              " 'si': 5,\n",
              " 'the': 3,\n",
              " 'there': 35,\n",
              " 'to': 46,\n",
              " 'took': 15,\n",
              " 'travelled': 17,\n",
              " 'up': 20,\n",
              " 'viajar': 28,\n",
              " 'went': 48}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i-BCqWxXnWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Separating each of the elements\n",
        "for story,question,answer in train_data:\n",
        "    train_story_text.append(story)\n",
        "    train_question_text.append(question) \n",
        "    train_answers.append(answer)\n",
        "    \n",
        "#Coverting the text into the indexes \n",
        "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP56LhzsXnYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a function for vectorizing the stories, questions and answers:\n",
        "def vectorize_stories(data,word_index = tokenizer.word_index, max_story_len = max_story_len, max_question_len = max_question_len):\n",
        "    #vectorized stories:\n",
        "    X = []\n",
        "    #vectorized questions:\n",
        "    Xq = []\n",
        "    #vectorized answers:\n",
        "    Y = []\n",
        "    new_word = \"no_ne\"\n",
        "    for story, question, answer in data:\n",
        "        #Getting indexes for each word in the story\n",
        "        #x = [word_index[word.lower()] for word in story]\n",
        "        x = []\n",
        "        for word in story:\n",
        "          \n",
        "          if word.lower() in word_index:\n",
        "            #print(word_index[word.lower()])\n",
        "            x.append(word_index[word.lower()])\n",
        "          else:\n",
        "            print(\"problema con nueva palabra\", word.lower() )\n",
        "            #print(word_index[new_word.lower()])\n",
        "            x.append(word_index[new_word.lower()])\n",
        "        #Getting indexes for each word in the story\n",
        "        xq = [word_index[word.lower()] for word in question]\n",
        "        #For the answers\n",
        "        y = np.zeros(len(word_index) + 1) #Index 0 Reserved when padding the sequences\n",
        "        y[word_index[answer]] = 1\n",
        "        \n",
        "        X.append(x)\n",
        "        Xq.append(xq)\n",
        "        Y.append(y)\n",
        "        \n",
        "    #Now we have to pad these sequences:\n",
        "    return(pad_sequences(X,maxlen=max_story_len), pad_sequences(Xq, maxlen=max_question_len), np.array(Y))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YuElFcylwbL",
        "colab_type": "code",
        "outputId": "662c6829-bf7c-4cd4-d07a-2dc00fea4d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "\n",
        "print(max_story_len)\n",
        "my_text = ' la_salida viajaria el date .'\n",
        "my_story = ' ellos ira en coche a londres .'\n",
        "my_story2 = ' ellos ir en coche a londres .'\n",
        "my_question = ' cuando llegar  ?'\n",
        "\n",
        "my_data = [(my_story.split(), my_question.split(),'no')]\n",
        "my_data2 = [(my_story2.split(), my_question.split(),'no')]\n",
        "#Vectorize this data\n",
        "my_story, my_ques, my_ans = vectorize_stories(my_data)\n",
        "print(my_story)\n",
        "print(\"*\"*30)\n",
        "my_story2, my_ques, my_ans = vectorize_stories(my_data2)\n",
        "\n",
        "print(my_story2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "156\n",
            "problema con nueva palabra ellos\n",
            "problema con nueva palabra ira\n",
            "problema con nueva palabra en\n",
            "problema con nueva palabra coche\n",
            "problema con nueva palabra a\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-402f63ce6464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmy_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_story2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'no'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Vectorize this data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmy_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_ques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_stories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_story\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-bb7a906261f3>\u001b[0m in \u001b[0;36mvectorize_stories\u001b[0;34m(data, word_index, max_story_len, max_question_len)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#Getting indexes for each word in the story\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mxq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m#For the answers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Index 0 Reserved when padding the sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-bb7a906261f3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#Getting indexes for each word in the story\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mxq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m#For the answers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Index 0 Reserved when padding the sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'llegar'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: Current TensorFlow version is 2.2.0-rc2. To use TF 1.x instead,\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\nyou run \"import tensorflow\".\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exBNVvkKXnYc",
        "colab_type": "code",
        "outputId": "d69d60bd-b5cd-4b22-8874-e658b2bac26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "inputs_train, questions_train, answers_train = vectorize_stories(train_data)\n",
        "inputs_test, questions_test, answers_test = vectorize_stories(test_data)\n",
        "print(\"----------------------------------------------------------\")\n",
        "inputs_train[0]\n",
        "print(\"----------------------------------------------------------\")\n",
        "train_story_text[0]\n",
        "print(\"----------------------------------------------------------\")\n",
        "train_story_seq[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f7e347423761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_stories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minputs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_stories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minputs_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-bb7a906261f3>\u001b[0m in \u001b[0;36mvectorize_stories\u001b[0;34m(data, word_index, max_story_len, max_question_len)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#For the answers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Index 0 Reserved when padding the sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'yes'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwcyEM7mXnYt",
        "colab_type": "text"
      },
      "source": [
        "## Building the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPPz91m2XnYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate, LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0flpxxUXnYx",
        "colab_type": "code",
        "outputId": "ec83ca10-8da0-4f55-cfe5-a887d0eb980d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# We need to create the placeholders \n",
        "#The Input function is used to create a keras tensor\n",
        "#PLACEHOLDER shape = (max_story_len,batch_size)\n",
        "#These are our placeholder for the inputs, ready to recieve batches of the stories and the questions\n",
        "input_sequence = Input((max_story_len,)) #As we dont know batch size yet\n",
        "question = Input((max_question_len,))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUT9JTL6XnY2",
        "colab_type": "code",
        "outputId": "6ba1a7fe-3990-47e7-b12f-cd77b905ca18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#Create input encoder M:\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_len,output_dim = 64)) #From paper\n",
        "input_encoder_m.add(Dropout(0.3))\n",
        "\n",
        "#Outputs: (Samples, story_maxlen,embedding_dim) -- Gives a list of the lenght of the samples where each item has the\n",
        "#lenght of the max story lenght and every word is embedded in the embbeding dimension"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ_a5v_rXnY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create input encoder C:\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_len,output_dim = max_question_len)) #From paper\n",
        "input_encoder_c.add(Dropout(0.3))\n",
        "\n",
        "#Outputs: (samples, story_maxlen, max_question_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1vtYKozXnZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create question encoder:\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_len,output_dim = 64,input_length=max_question_len)) #From paper\n",
        "question_encoder.add(Dropout(0.3))\n",
        "\n",
        "#Outputs: (samples, question_maxlen, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1wnlDh_XnZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now lets encode the sequences, passing the placeholders into our encoders:\n",
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br1_oze5XnZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use dot product to compute similarity between input encoded m and question \n",
        "#Like in the paper:\n",
        "match = dot([input_encoded_m,question_encoded], axes = (2,2))\n",
        "match = Activation('softmax')(match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkVpTpXmXnZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For the response we want to add this match with the ouput of input_encoded_c\n",
        "response = add([match,input_encoded_c])\n",
        "response = Permute((2,1))(response) #Permute Layer: permutes dimensions of input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk2RKObVXnZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Once we have the response we can concatenate it with the question encoded:\n",
        "answer = concatenate([response, question_encoded])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMSNqVVTXnZX",
        "colab_type": "code",
        "outputId": "17e9035a-a39d-480e-8f76-4a23851942db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "answer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'concatenate_1/concat:0' shape=(?, 3, 69) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbp56anVXnZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce the answer tensor with a RNN (LSTM)\n",
        "answer = LSTM(32)(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB4efTzfXnZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Regularization with dropout:\n",
        "answer = Dropout(0.5)(answer)\n",
        "#Output layer:\n",
        "answer = Dense(vocab_len)(answer) #Output shape: (Samples, Vocab_size) #Yes or no and all 0s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q-rJG9-XnZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we need to output a probability distribution for the vocab, using softmax:\n",
        "answer = Activation('softmax')(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCE3PjmuXnZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we build the final model:\n",
        "model = Model([input_sequence,question], answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFVnUX3GXnZm",
        "colab_type": "code",
        "outputId": "62e699b8-fd95-4dde-8db7-e406370ba2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "#Categorical instead of binary cross entropy as because of the way we are training\n",
        "#we could actually see any of the words from the vocab as output\n",
        "#however, we should only see yes or no"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjSpvbf-XnZp",
        "colab_type": "code",
        "outputId": "a46b26fb-2e0c-4eae-aaf4-a9048aae5f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 5)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 3)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       multiple             5056        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 3, 64)        5056        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 5, 3)         0           sequential_1[1][0]               \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 5, 3)         0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       multiple             237         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 5, 3)         0           activation_1[0][0]               \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 3, 5)         0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3, 69)        0           permute_1[0][0]                  \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 32)           13056       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 79)           2607        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 79)           0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 26,012\n",
            "Trainable params: 26,012\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n52JDoFsXnZ7",
        "colab_type": "text"
      },
      "source": [
        "## Training and testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0dZc6gRXnZ8",
        "colab_type": "code",
        "outputId": "8d476dec-b99e-4667-baa8-f038b2b40a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit([inputs_train,questions_train],answers_train, batch_size = 32, epochs =450, validation_data = ([inputs_test,questions_test],answers_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/450\n",
            "10000/10000 [==============================] - 3s 334us/step - loss: 2.1150 - acc: 0.4737 - val_loss: 1.0978 - val_acc: 0.7410\n",
            "Epoch 2/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.9884 - acc: 0.7433 - val_loss: 0.7465 - val_acc: 0.7410\n",
            "Epoch 3/450\n",
            "10000/10000 [==============================] - 2s 185us/step - loss: 0.7230 - acc: 0.7542 - val_loss: 0.5650 - val_acc: 0.7440\n",
            "Epoch 4/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.5672 - acc: 0.7968 - val_loss: 0.4168 - val_acc: 0.8800\n",
            "Epoch 5/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.4518 - acc: 0.8500 - val_loss: 0.3404 - val_acc: 0.8800\n",
            "Epoch 6/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.3951 - acc: 0.8680 - val_loss: 0.3182 - val_acc: 0.8840\n",
            "Epoch 7/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.3599 - acc: 0.8788 - val_loss: 0.2932 - val_acc: 0.8840\n",
            "Epoch 8/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.3290 - acc: 0.8844 - val_loss: 0.2707 - val_acc: 0.9090\n",
            "Epoch 9/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.3023 - acc: 0.8927 - val_loss: 0.2510 - val_acc: 0.9080\n",
            "Epoch 10/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.2834 - acc: 0.8982 - val_loss: 0.2359 - val_acc: 0.9290\n",
            "Epoch 11/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.2657 - acc: 0.9065 - val_loss: 0.2179 - val_acc: 0.9460\n",
            "Epoch 12/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.2537 - acc: 0.9062 - val_loss: 0.2048 - val_acc: 0.9360\n",
            "Epoch 13/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.2403 - acc: 0.9103 - val_loss: 0.1894 - val_acc: 0.9370\n",
            "Epoch 14/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.2259 - acc: 0.9170 - val_loss: 0.1820 - val_acc: 0.9410\n",
            "Epoch 15/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.2157 - acc: 0.9209 - val_loss: 0.1677 - val_acc: 0.9610\n",
            "Epoch 16/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.2049 - acc: 0.9255 - val_loss: 0.1584 - val_acc: 0.9610\n",
            "Epoch 17/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.1943 - acc: 0.9331 - val_loss: 0.1459 - val_acc: 0.9640\n",
            "Epoch 18/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.1805 - acc: 0.9399 - val_loss: 0.1339 - val_acc: 0.9800\n",
            "Epoch 19/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.1743 - acc: 0.9440 - val_loss: 0.1213 - val_acc: 0.9680\n",
            "Epoch 20/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.1586 - acc: 0.9475 - val_loss: 0.1110 - val_acc: 0.9750\n",
            "Epoch 21/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.1506 - acc: 0.9532 - val_loss: 0.0988 - val_acc: 0.9820\n",
            "Epoch 22/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.1431 - acc: 0.9561 - val_loss: 0.0872 - val_acc: 0.9820\n",
            "Epoch 23/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.1341 - acc: 0.9585 - val_loss: 0.0774 - val_acc: 0.9960\n",
            "Epoch 24/450\n",
            "10000/10000 [==============================] - 2s 185us/step - loss: 0.1287 - acc: 0.9597 - val_loss: 0.0681 - val_acc: 1.0000\n",
            "Epoch 25/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.1247 - acc: 0.9648 - val_loss: 0.0613 - val_acc: 1.0000\n",
            "Epoch 26/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.1123 - acc: 0.9671 - val_loss: 0.0557 - val_acc: 1.0000\n",
            "Epoch 27/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.1046 - acc: 0.9736 - val_loss: 0.0455 - val_acc: 1.0000\n",
            "Epoch 28/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.1010 - acc: 0.9727 - val_loss: 0.0431 - val_acc: 1.0000\n",
            "Epoch 29/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0964 - acc: 0.9746 - val_loss: 0.0346 - val_acc: 1.0000\n",
            "Epoch 30/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0902 - acc: 0.9774 - val_loss: 0.0328 - val_acc: 1.0000\n",
            "Epoch 31/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0896 - acc: 0.9752 - val_loss: 0.0259 - val_acc: 1.0000\n",
            "Epoch 32/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0817 - acc: 0.9790 - val_loss: 0.0247 - val_acc: 1.0000\n",
            "Epoch 33/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0813 - acc: 0.9776 - val_loss: 0.0217 - val_acc: 1.0000\n",
            "Epoch 34/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0721 - acc: 0.9809 - val_loss: 0.0193 - val_acc: 1.0000\n",
            "Epoch 35/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0745 - acc: 0.9799 - val_loss: 0.0177 - val_acc: 1.0000\n",
            "Epoch 36/450\n",
            "10000/10000 [==============================] - 2s 184us/step - loss: 0.0736 - acc: 0.9789 - val_loss: 0.0147 - val_acc: 1.0000\n",
            "Epoch 37/450\n",
            "10000/10000 [==============================] - 2s 204us/step - loss: 0.0663 - acc: 0.9826 - val_loss: 0.0139 - val_acc: 1.0000\n",
            "Epoch 38/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0711 - acc: 0.9792 - val_loss: 0.0125 - val_acc: 1.0000\n",
            "Epoch 39/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0656 - acc: 0.9820 - val_loss: 0.0114 - val_acc: 1.0000\n",
            "Epoch 40/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0662 - acc: 0.9816 - val_loss: 0.0104 - val_acc: 1.0000\n",
            "Epoch 41/450\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.0617 - acc: 0.9827 - val_loss: 0.0096 - val_acc: 1.0000\n",
            "Epoch 42/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0603 - acc: 0.9829 - val_loss: 0.0083 - val_acc: 1.0000\n",
            "Epoch 43/450\n",
            "10000/10000 [==============================] - 2s 184us/step - loss: 0.0559 - acc: 0.9847 - val_loss: 0.0078 - val_acc: 1.0000\n",
            "Epoch 44/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0526 - acc: 0.9860 - val_loss: 0.0073 - val_acc: 1.0000\n",
            "Epoch 45/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0579 - acc: 0.9853 - val_loss: 0.0065 - val_acc: 1.0000\n",
            "Epoch 46/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0525 - acc: 0.9867 - val_loss: 0.0061 - val_acc: 1.0000\n",
            "Epoch 47/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0510 - acc: 0.9864 - val_loss: 0.0054 - val_acc: 1.0000\n",
            "Epoch 48/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0478 - acc: 0.9874 - val_loss: 0.0050 - val_acc: 1.0000\n",
            "Epoch 49/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0515 - acc: 0.9859 - val_loss: 0.0046 - val_acc: 1.0000\n",
            "Epoch 50/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0511 - acc: 0.9841 - val_loss: 0.0043 - val_acc: 1.0000\n",
            "Epoch 51/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0467 - acc: 0.9862 - val_loss: 0.0043 - val_acc: 1.0000\n",
            "Epoch 52/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0433 - acc: 0.9883 - val_loss: 0.0034 - val_acc: 1.0000\n",
            "Epoch 53/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0426 - acc: 0.9897 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 54/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0440 - acc: 0.9874 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 55/450\n",
            "10000/10000 [==============================] - 2s 185us/step - loss: 0.0436 - acc: 0.9872 - val_loss: 0.0030 - val_acc: 1.0000\n",
            "Epoch 56/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0440 - acc: 0.9881 - val_loss: 0.0027 - val_acc: 1.0000\n",
            "Epoch 57/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0390 - acc: 0.9895 - val_loss: 0.0025 - val_acc: 1.0000\n",
            "Epoch 58/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0386 - acc: 0.9889 - val_loss: 0.0022 - val_acc: 1.0000\n",
            "Epoch 59/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0396 - acc: 0.9888 - val_loss: 0.0026 - val_acc: 1.0000\n",
            "Epoch 60/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0404 - acc: 0.9889 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 61/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0383 - acc: 0.9898 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 62/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0400 - acc: 0.9897 - val_loss: 0.0017 - val_acc: 1.0000\n",
            "Epoch 63/450\n",
            "10000/10000 [==============================] - 2s 185us/step - loss: 0.0327 - acc: 0.9914 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 64/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0335 - acc: 0.9908 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 65/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0359 - acc: 0.9892 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 66/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0363 - acc: 0.9907 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 67/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0339 - acc: 0.9908 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 68/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0338 - acc: 0.9912 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 69/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0282 - acc: 0.9916 - val_loss: 0.0010 - val_acc: 1.0000\n",
            "Epoch 70/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0356 - acc: 0.9894 - val_loss: 9.9713e-04 - val_acc: 1.0000\n",
            "Epoch 71/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0306 - acc: 0.9913 - val_loss: 9.7885e-04 - val_acc: 1.0000\n",
            "Epoch 72/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0315 - acc: 0.9923 - val_loss: 8.6924e-04 - val_acc: 1.0000\n",
            "Epoch 73/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0293 - acc: 0.9914 - val_loss: 7.8213e-04 - val_acc: 1.0000\n",
            "Epoch 74/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0276 - acc: 0.9929 - val_loss: 7.2302e-04 - val_acc: 1.0000\n",
            "Epoch 75/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0259 - acc: 0.9927 - val_loss: 7.0503e-04 - val_acc: 1.0000\n",
            "Epoch 76/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0278 - acc: 0.9919 - val_loss: 6.4067e-04 - val_acc: 1.0000\n",
            "Epoch 77/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0278 - acc: 0.9917 - val_loss: 6.3243e-04 - val_acc: 1.0000\n",
            "Epoch 78/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0284 - acc: 0.9930 - val_loss: 5.8423e-04 - val_acc: 1.0000\n",
            "Epoch 79/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0298 - acc: 0.9921 - val_loss: 6.6887e-04 - val_acc: 1.0000\n",
            "Epoch 80/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0269 - acc: 0.9918 - val_loss: 5.3984e-04 - val_acc: 1.0000\n",
            "Epoch 81/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0240 - acc: 0.9936 - val_loss: 5.2434e-04 - val_acc: 1.0000\n",
            "Epoch 82/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0282 - acc: 0.9930 - val_loss: 4.5619e-04 - val_acc: 1.0000\n",
            "Epoch 83/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0230 - acc: 0.9929 - val_loss: 4.0067e-04 - val_acc: 1.0000\n",
            "Epoch 84/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0242 - acc: 0.9939 - val_loss: 3.8190e-04 - val_acc: 1.0000\n",
            "Epoch 85/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0270 - acc: 0.9922 - val_loss: 4.2893e-04 - val_acc: 1.0000\n",
            "Epoch 86/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0287 - acc: 0.9912 - val_loss: 3.8004e-04 - val_acc: 1.0000\n",
            "Epoch 87/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0242 - acc: 0.9936 - val_loss: 3.2802e-04 - val_acc: 1.0000\n",
            "Epoch 88/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0231 - acc: 0.9935 - val_loss: 3.1076e-04 - val_acc: 1.0000\n",
            "Epoch 89/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0251 - acc: 0.9935 - val_loss: 3.0785e-04 - val_acc: 1.0000\n",
            "Epoch 90/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0246 - acc: 0.9938 - val_loss: 3.0630e-04 - val_acc: 1.0000\n",
            "Epoch 91/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0246 - acc: 0.9933 - val_loss: 2.6402e-04 - val_acc: 1.0000\n",
            "Epoch 92/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0237 - acc: 0.9943 - val_loss: 2.5130e-04 - val_acc: 1.0000\n",
            "Epoch 93/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 2.4148e-04 - val_acc: 1.0000\n",
            "Epoch 94/450\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.0209 - acc: 0.9945 - val_loss: 2.8535e-04 - val_acc: 1.0000\n",
            "Epoch 95/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0197 - acc: 0.9950 - val_loss: 2.3369e-04 - val_acc: 1.0000\n",
            "Epoch 96/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0201 - acc: 0.9945 - val_loss: 1.9118e-04 - val_acc: 1.0000\n",
            "Epoch 97/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0203 - acc: 0.9943 - val_loss: 2.4963e-04 - val_acc: 1.0000\n",
            "Epoch 98/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0176 - acc: 0.9949 - val_loss: 1.7815e-04 - val_acc: 1.0000\n",
            "Epoch 99/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0244 - acc: 0.9931 - val_loss: 2.0684e-04 - val_acc: 1.0000\n",
            "Epoch 100/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0185 - acc: 0.9956 - val_loss: 1.6587e-04 - val_acc: 1.0000\n",
            "Epoch 101/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0197 - acc: 0.9952 - val_loss: 1.4327e-04 - val_acc: 1.0000\n",
            "Epoch 102/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0193 - acc: 0.9947 - val_loss: 1.6234e-04 - val_acc: 1.0000\n",
            "Epoch 103/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0193 - acc: 0.9948 - val_loss: 1.2812e-04 - val_acc: 1.0000\n",
            "Epoch 104/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0215 - acc: 0.9933 - val_loss: 1.4520e-04 - val_acc: 1.0000\n",
            "Epoch 105/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0215 - acc: 0.9946 - val_loss: 1.2775e-04 - val_acc: 1.0000\n",
            "Epoch 106/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0168 - acc: 0.9958 - val_loss: 1.1955e-04 - val_acc: 1.0000\n",
            "Epoch 107/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0212 - acc: 0.9948 - val_loss: 1.2325e-04 - val_acc: 1.0000\n",
            "Epoch 108/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0182 - acc: 0.9951 - val_loss: 1.0420e-04 - val_acc: 1.0000\n",
            "Epoch 109/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0201 - acc: 0.9942 - val_loss: 1.0397e-04 - val_acc: 1.0000\n",
            "Epoch 110/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0160 - acc: 0.9949 - val_loss: 9.9612e-05 - val_acc: 1.0000\n",
            "Epoch 111/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0185 - acc: 0.9949 - val_loss: 9.9203e-05 - val_acc: 1.0000\n",
            "Epoch 112/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0199 - acc: 0.9951 - val_loss: 8.9152e-05 - val_acc: 1.0000\n",
            "Epoch 113/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0180 - acc: 0.9952 - val_loss: 8.3259e-05 - val_acc: 1.0000\n",
            "Epoch 114/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0192 - acc: 0.9941 - val_loss: 8.9588e-05 - val_acc: 1.0000\n",
            "Epoch 115/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0187 - acc: 0.9942 - val_loss: 8.8680e-05 - val_acc: 1.0000\n",
            "Epoch 116/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0204 - acc: 0.9936 - val_loss: 7.8934e-05 - val_acc: 1.0000\n",
            "Epoch 117/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0173 - acc: 0.9949 - val_loss: 8.1177e-05 - val_acc: 1.0000\n",
            "Epoch 118/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 8.4790e-05 - val_acc: 1.0000\n",
            "Epoch 119/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0188 - acc: 0.9954 - val_loss: 8.0011e-05 - val_acc: 1.0000\n",
            "Epoch 120/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0155 - acc: 0.9956 - val_loss: 7.3369e-05 - val_acc: 1.0000\n",
            "Epoch 121/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 7.3336e-05 - val_acc: 1.0000\n",
            "Epoch 122/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0197 - acc: 0.9948 - val_loss: 6.7164e-05 - val_acc: 1.0000\n",
            "Epoch 123/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 8.0728e-05 - val_acc: 1.0000\n",
            "Epoch 124/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0161 - acc: 0.9945 - val_loss: 6.1429e-05 - val_acc: 1.0000\n",
            "Epoch 125/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0151 - acc: 0.9958 - val_loss: 6.7226e-05 - val_acc: 1.0000\n",
            "Epoch 126/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0154 - acc: 0.9957 - val_loss: 5.6438e-05 - val_acc: 1.0000\n",
            "Epoch 127/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0150 - acc: 0.9964 - val_loss: 5.0213e-05 - val_acc: 1.0000\n",
            "Epoch 128/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 5.0392e-05 - val_acc: 1.0000\n",
            "Epoch 129/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0166 - acc: 0.9956 - val_loss: 4.7969e-05 - val_acc: 1.0000\n",
            "Epoch 130/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0145 - acc: 0.9962 - val_loss: 4.1559e-05 - val_acc: 1.0000\n",
            "Epoch 131/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0142 - acc: 0.9964 - val_loss: 3.7125e-05 - val_acc: 1.0000\n",
            "Epoch 132/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0173 - acc: 0.9943 - val_loss: 3.8151e-05 - val_acc: 1.0000\n",
            "Epoch 133/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0173 - acc: 0.9941 - val_loss: 4.1000e-05 - val_acc: 1.0000\n",
            "Epoch 134/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0158 - acc: 0.9960 - val_loss: 3.2631e-05 - val_acc: 1.0000\n",
            "Epoch 135/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0139 - acc: 0.9967 - val_loss: 3.1866e-05 - val_acc: 1.0000\n",
            "Epoch 136/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0131 - acc: 0.9965 - val_loss: 2.9856e-05 - val_acc: 1.0000\n",
            "Epoch 137/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0163 - acc: 0.9950 - val_loss: 2.8525e-05 - val_acc: 1.0000\n",
            "Epoch 138/450\n",
            "10000/10000 [==============================] - 2s 205us/step - loss: 0.0136 - acc: 0.9961 - val_loss: 2.6925e-05 - val_acc: 1.0000\n",
            "Epoch 139/450\n",
            "10000/10000 [==============================] - 2s 203us/step - loss: 0.0169 - acc: 0.9946 - val_loss: 2.6760e-05 - val_acc: 1.0000\n",
            "Epoch 140/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0148 - acc: 0.9962 - val_loss: 3.0082e-05 - val_acc: 1.0000\n",
            "Epoch 141/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0125 - acc: 0.9965 - val_loss: 2.4820e-05 - val_acc: 1.0000\n",
            "Epoch 142/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0125 - acc: 0.9965 - val_loss: 2.2901e-05 - val_acc: 1.0000\n",
            "Epoch 143/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0126 - acc: 0.9962 - val_loss: 2.3427e-05 - val_acc: 1.0000\n",
            "Epoch 144/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0153 - acc: 0.9957 - val_loss: 2.2243e-05 - val_acc: 1.0000\n",
            "Epoch 145/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0130 - acc: 0.9959 - val_loss: 2.4782e-05 - val_acc: 1.0000\n",
            "Epoch 146/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0135 - acc: 0.9961 - val_loss: 2.1684e-05 - val_acc: 1.0000\n",
            "Epoch 147/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0127 - acc: 0.9962 - val_loss: 2.7626e-05 - val_acc: 1.0000\n",
            "Epoch 148/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0102 - acc: 0.9973 - val_loss: 2.0843e-05 - val_acc: 1.0000\n",
            "Epoch 149/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0128 - acc: 0.9968 - val_loss: 2.0539e-05 - val_acc: 1.0000\n",
            "Epoch 150/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0125 - acc: 0.9954 - val_loss: 1.9631e-05 - val_acc: 1.0000\n",
            "Epoch 151/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0150 - acc: 0.9958 - val_loss: 1.9527e-05 - val_acc: 1.0000\n",
            "Epoch 152/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0135 - acc: 0.9962 - val_loss: 1.9640e-05 - val_acc: 1.0000\n",
            "Epoch 153/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0134 - acc: 0.9965 - val_loss: 1.8199e-05 - val_acc: 1.0000\n",
            "Epoch 154/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0123 - acc: 0.9962 - val_loss: 1.6877e-05 - val_acc: 1.0000\n",
            "Epoch 155/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0112 - acc: 0.9962 - val_loss: 1.5839e-05 - val_acc: 1.0000\n",
            "Epoch 156/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0134 - acc: 0.9958 - val_loss: 1.5550e-05 - val_acc: 1.0000\n",
            "Epoch 157/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0135 - acc: 0.9965 - val_loss: 1.3587e-05 - val_acc: 1.0000\n",
            "Epoch 158/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0127 - acc: 0.9967 - val_loss: 1.2353e-05 - val_acc: 1.0000\n",
            "Epoch 159/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0122 - acc: 0.9963 - val_loss: 1.3152e-05 - val_acc: 1.0000\n",
            "Epoch 160/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0111 - acc: 0.9966 - val_loss: 1.2425e-05 - val_acc: 1.0000\n",
            "Epoch 161/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0105 - acc: 0.9964 - val_loss: 1.2563e-05 - val_acc: 1.0000\n",
            "Epoch 162/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 1.2116e-05 - val_acc: 1.0000\n",
            "Epoch 163/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0140 - acc: 0.9957 - val_loss: 1.2146e-05 - val_acc: 1.0000\n",
            "Epoch 164/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0113 - acc: 0.9961 - val_loss: 1.1870e-05 - val_acc: 1.0000\n",
            "Epoch 165/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0155 - acc: 0.9955 - val_loss: 1.2863e-05 - val_acc: 1.0000\n",
            "Epoch 166/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0117 - acc: 0.9967 - val_loss: 1.2940e-05 - val_acc: 1.0000\n",
            "Epoch 167/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0134 - acc: 0.9960 - val_loss: 1.0840e-05 - val_acc: 1.0000\n",
            "Epoch 168/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0126 - acc: 0.9974 - val_loss: 9.8026e-06 - val_acc: 1.0000\n",
            "Epoch 169/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0115 - acc: 0.9967 - val_loss: 1.1162e-05 - val_acc: 1.0000\n",
            "Epoch 170/450\n",
            "10000/10000 [==============================] - 2s 184us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 1.0157e-05 - val_acc: 1.0000\n",
            "Epoch 171/450\n",
            "10000/10000 [==============================] - 2s 183us/step - loss: 0.0125 - acc: 0.9963 - val_loss: 1.2395e-05 - val_acc: 1.0000\n",
            "Epoch 172/450\n",
            "10000/10000 [==============================] - 2s 184us/step - loss: 0.0141 - acc: 0.9959 - val_loss: 9.7308e-06 - val_acc: 1.0000\n",
            "Epoch 173/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0138 - acc: 0.9964 - val_loss: 9.4884e-06 - val_acc: 1.0000\n",
            "Epoch 174/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0102 - acc: 0.9973 - val_loss: 8.9209e-06 - val_acc: 1.0000\n",
            "Epoch 175/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0102 - acc: 0.9971 - val_loss: 7.9882e-06 - val_acc: 1.0000\n",
            "Epoch 176/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0105 - acc: 0.9969 - val_loss: 8.1587e-06 - val_acc: 1.0000\n",
            "Epoch 177/450\n",
            "10000/10000 [==============================] - 2s 183us/step - loss: 0.0140 - acc: 0.9953 - val_loss: 9.9955e-06 - val_acc: 1.0000\n",
            "Epoch 178/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0098 - acc: 0.9972 - val_loss: 7.3839e-06 - val_acc: 1.0000\n",
            "Epoch 179/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0106 - acc: 0.9967 - val_loss: 6.9782e-06 - val_acc: 1.0000\n",
            "Epoch 180/450\n",
            "10000/10000 [==============================] - 2s 185us/step - loss: 0.0074 - acc: 0.9981 - val_loss: 6.8510e-06 - val_acc: 1.0000\n",
            "Epoch 181/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0094 - acc: 0.9971 - val_loss: 6.2644e-06 - val_acc: 1.0000\n",
            "Epoch 182/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0087 - acc: 0.9977 - val_loss: 6.2969e-06 - val_acc: 1.0000\n",
            "Epoch 183/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 5.6000e-06 - val_acc: 1.0000\n",
            "Epoch 184/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 5.2120e-06 - val_acc: 1.0000\n",
            "Epoch 185/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0091 - acc: 0.9978 - val_loss: 4.6141e-06 - val_acc: 1.0000\n",
            "Epoch 186/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0089 - acc: 0.9973 - val_loss: 5.2928e-06 - val_acc: 1.0000\n",
            "Epoch 187/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 5.9363e-06 - val_acc: 1.0000\n",
            "Epoch 188/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0105 - acc: 0.9972 - val_loss: 4.3675e-06 - val_acc: 1.0000\n",
            "Epoch 189/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0098 - acc: 0.9971 - val_loss: 4.8707e-06 - val_acc: 1.0000\n",
            "Epoch 190/450\n",
            "10000/10000 [==============================] - 2s 203us/step - loss: 0.0085 - acc: 0.9982 - val_loss: 4.6289e-06 - val_acc: 1.0000\n",
            "Epoch 191/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0075 - acc: 0.9976 - val_loss: 4.9902e-06 - val_acc: 1.0000\n",
            "Epoch 192/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0077 - acc: 0.9973 - val_loss: 4.3236e-06 - val_acc: 1.0000\n",
            "Epoch 193/450\n",
            "10000/10000 [==============================] - 2s 184us/step - loss: 0.0070 - acc: 0.9986 - val_loss: 3.8816e-06 - val_acc: 1.0000\n",
            "Epoch 194/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0075 - acc: 0.9978 - val_loss: 3.9471e-06 - val_acc: 1.0000\n",
            "Epoch 195/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0110 - acc: 0.9968 - val_loss: 5.7982e-06 - val_acc: 1.0000\n",
            "Epoch 196/450\n",
            "10000/10000 [==============================] - 2s 184us/step - loss: 0.0075 - acc: 0.9984 - val_loss: 3.8541e-06 - val_acc: 1.0000\n",
            "Epoch 197/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 3.6234e-06 - val_acc: 1.0000\n",
            "Epoch 198/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0076 - acc: 0.9980 - val_loss: 3.4399e-06 - val_acc: 1.0000\n",
            "Epoch 199/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0093 - acc: 0.9969 - val_loss: 3.6545e-06 - val_acc: 1.0000\n",
            "Epoch 200/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 3.0690e-06 - val_acc: 1.0000\n",
            "Epoch 201/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0076 - acc: 0.9972 - val_loss: 2.8926e-06 - val_acc: 1.0000\n",
            "Epoch 202/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0079 - acc: 0.9979 - val_loss: 2.6911e-06 - val_acc: 1.0000\n",
            "Epoch 203/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0082 - acc: 0.9972 - val_loss: 2.7425e-06 - val_acc: 1.0000\n",
            "Epoch 204/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 2.5644e-06 - val_acc: 1.0000\n",
            "Epoch 205/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 3.0413e-06 - val_acc: 1.0000\n",
            "Epoch 206/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0059 - acc: 0.9983 - val_loss: 2.2346e-06 - val_acc: 1.0000\n",
            "Epoch 207/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0094 - acc: 0.9972 - val_loss: 2.1265e-06 - val_acc: 1.0000\n",
            "Epoch 208/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 2.2830e-06 - val_acc: 1.0000\n",
            "Epoch 209/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0066 - acc: 0.9975 - val_loss: 2.3144e-06 - val_acc: 1.0000\n",
            "Epoch 210/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 2.0537e-06 - val_acc: 1.0000\n",
            "Epoch 211/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0075 - acc: 0.9985 - val_loss: 1.9662e-06 - val_acc: 1.0000\n",
            "Epoch 212/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0072 - acc: 0.9983 - val_loss: 1.9395e-06 - val_acc: 1.0000\n",
            "Epoch 213/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0064 - acc: 0.9986 - val_loss: 1.8727e-06 - val_acc: 1.0000\n",
            "Epoch 214/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0081 - acc: 0.9969 - val_loss: 1.7724e-06 - val_acc: 1.0000\n",
            "Epoch 215/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0060 - acc: 0.9981 - val_loss: 1.6952e-06 - val_acc: 1.0000\n",
            "Epoch 216/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0056 - acc: 0.9983 - val_loss: 2.0263e-06 - val_acc: 1.0000\n",
            "Epoch 217/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0108 - acc: 0.9973 - val_loss: 1.7799e-06 - val_acc: 1.0000\n",
            "Epoch 218/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 1.7146e-06 - val_acc: 1.0000\n",
            "Epoch 219/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0077 - acc: 0.9980 - val_loss: 1.6003e-06 - val_acc: 1.0000\n",
            "Epoch 220/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0077 - acc: 0.9979 - val_loss: 1.5351e-06 - val_acc: 1.0000\n",
            "Epoch 221/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0056 - acc: 0.9982 - val_loss: 1.4920e-06 - val_acc: 1.0000\n",
            "Epoch 222/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0102 - acc: 0.9971 - val_loss: 1.4284e-06 - val_acc: 1.0000\n",
            "Epoch 223/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0055 - acc: 0.9981 - val_loss: 1.3458e-06 - val_acc: 1.0000\n",
            "Epoch 224/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0068 - acc: 0.9985 - val_loss: 1.3656e-06 - val_acc: 1.0000\n",
            "Epoch 225/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0079 - acc: 0.9973 - val_loss: 1.2936e-06 - val_acc: 1.0000\n",
            "Epoch 226/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0050 - acc: 0.9984 - val_loss: 1.4052e-06 - val_acc: 1.0000\n",
            "Epoch 227/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 1.3498e-06 - val_acc: 1.0000\n",
            "Epoch 228/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0078 - acc: 0.9978 - val_loss: 1.3816e-06 - val_acc: 1.0000\n",
            "Epoch 229/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0083 - acc: 0.9980 - val_loss: 1.2860e-06 - val_acc: 1.0000\n",
            "Epoch 230/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0099 - acc: 0.9975 - val_loss: 1.3224e-06 - val_acc: 1.0000\n",
            "Epoch 231/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0067 - acc: 0.9977 - val_loss: 1.3775e-06 - val_acc: 1.0000\n",
            "Epoch 232/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0089 - acc: 0.9977 - val_loss: 1.2115e-06 - val_acc: 1.0000\n",
            "Epoch 233/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0059 - acc: 0.9982 - val_loss: 1.1064e-06 - val_acc: 1.0000\n",
            "Epoch 234/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0072 - acc: 0.9977 - val_loss: 1.1177e-06 - val_acc: 1.0000\n",
            "Epoch 235/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0048 - acc: 0.9983 - val_loss: 1.0087e-06 - val_acc: 1.0000\n",
            "Epoch 236/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0058 - acc: 0.9982 - val_loss: 1.1338e-06 - val_acc: 1.0000\n",
            "Epoch 237/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0068 - acc: 0.9977 - val_loss: 1.0565e-06 - val_acc: 1.0000\n",
            "Epoch 238/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0072 - acc: 0.9975 - val_loss: 1.2618e-06 - val_acc: 1.0000\n",
            "Epoch 239/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0074 - acc: 0.9990 - val_loss: 9.5279e-07 - val_acc: 1.0000\n",
            "Epoch 240/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0080 - acc: 0.9974 - val_loss: 8.3280e-07 - val_acc: 1.0000\n",
            "Epoch 241/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0066 - acc: 0.9982 - val_loss: 7.8220e-07 - val_acc: 1.0000\n",
            "Epoch 242/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0061 - acc: 0.9980 - val_loss: 8.7697e-07 - val_acc: 1.0000\n",
            "Epoch 243/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0049 - acc: 0.9987 - val_loss: 8.2946e-07 - val_acc: 1.0000\n",
            "Epoch 244/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0063 - acc: 0.9981 - val_loss: 8.5730e-07 - val_acc: 1.0000\n",
            "Epoch 245/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0044 - acc: 0.9983 - val_loss: 7.6551e-07 - val_acc: 1.0000\n",
            "Epoch 246/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 7.9209e-07 - val_acc: 1.0000\n",
            "Epoch 247/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0060 - acc: 0.9979 - val_loss: 8.0276e-07 - val_acc: 1.0000\n",
            "Epoch 248/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0075 - acc: 0.9977 - val_loss: 6.8015e-07 - val_acc: 1.0000\n",
            "Epoch 249/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0067 - acc: 0.9977 - val_loss: 7.2176e-07 - val_acc: 1.0000\n",
            "Epoch 250/450\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.0053 - acc: 0.9983 - val_loss: 6.6966e-07 - val_acc: 1.0000\n",
            "Epoch 251/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0079 - acc: 0.9977 - val_loss: 7.5120e-07 - val_acc: 1.0000\n",
            "Epoch 252/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0069 - acc: 0.9982 - val_loss: 7.1317e-07 - val_acc: 1.0000\n",
            "Epoch 253/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0067 - acc: 0.9982 - val_loss: 6.9374e-07 - val_acc: 1.0000\n",
            "Epoch 254/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0079 - acc: 0.9981 - val_loss: 5.6941e-07 - val_acc: 1.0000\n",
            "Epoch 255/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 5.9057e-07 - val_acc: 1.0000\n",
            "Epoch 256/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 5.7954e-07 - val_acc: 1.0000\n",
            "Epoch 257/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0082 - acc: 0.9981 - val_loss: 5.2923e-07 - val_acc: 1.0000\n",
            "Epoch 258/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0057 - acc: 0.9983 - val_loss: 5.0879e-07 - val_acc: 1.0000\n",
            "Epoch 259/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0052 - acc: 0.9989 - val_loss: 4.8524e-07 - val_acc: 1.0000\n",
            "Epoch 260/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0089 - acc: 0.9981 - val_loss: 4.1354e-07 - val_acc: 1.0000\n",
            "Epoch 261/450\n",
            "10000/10000 [==============================] - 2s 187us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 4.3589e-07 - val_acc: 1.0000\n",
            "Epoch 262/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 4.8512e-07 - val_acc: 1.0000\n",
            "Epoch 263/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0048 - acc: 0.9990 - val_loss: 4.5610e-07 - val_acc: 1.0000\n",
            "Epoch 264/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 5.1028e-07 - val_acc: 1.0000\n",
            "Epoch 265/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 5.3162e-07 - val_acc: 1.0000\n",
            "Epoch 266/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 3.7068e-07 - val_acc: 1.0000\n",
            "Epoch 267/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0055 - acc: 0.9986 - val_loss: 3.6174e-07 - val_acc: 1.0000\n",
            "Epoch 268/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0090 - acc: 0.9978 - val_loss: 3.8576e-07 - val_acc: 1.0000\n",
            "Epoch 269/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 4.1050e-07 - val_acc: 1.0000\n",
            "Epoch 270/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0083 - acc: 0.9975 - val_loss: 3.6138e-07 - val_acc: 1.0000\n",
            "Epoch 271/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0069 - acc: 0.9978 - val_loss: 3.6156e-07 - val_acc: 1.0000\n",
            "Epoch 272/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0087 - acc: 0.9979 - val_loss: 4.2480e-07 - val_acc: 1.0000\n",
            "Epoch 273/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0040 - acc: 0.9986 - val_loss: 4.3929e-07 - val_acc: 1.0000\n",
            "Epoch 274/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0076 - acc: 0.9983 - val_loss: 3.6967e-07 - val_acc: 1.0000\n",
            "Epoch 275/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0073 - acc: 0.9977 - val_loss: 3.5012e-07 - val_acc: 1.0000\n",
            "Epoch 276/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0045 - acc: 0.9987 - val_loss: 3.2449e-07 - val_acc: 1.0000\n",
            "Epoch 277/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 3.1662e-07 - val_acc: 1.0000\n",
            "Epoch 278/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 3.7122e-07 - val_acc: 1.0000\n",
            "Epoch 279/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0049 - acc: 0.9987 - val_loss: 3.1823e-07 - val_acc: 1.0000\n",
            "Epoch 280/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 3.1149e-07 - val_acc: 1.0000\n",
            "Epoch 281/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0069 - acc: 0.9985 - val_loss: 3.5203e-07 - val_acc: 1.0000\n",
            "Epoch 282/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0092 - acc: 0.9983 - val_loss: 3.2550e-07 - val_acc: 1.0000\n",
            "Epoch 283/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 2.9475e-07 - val_acc: 1.0000\n",
            "Epoch 284/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0049 - acc: 0.9983 - val_loss: 3.1447e-07 - val_acc: 1.0000\n",
            "Epoch 285/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0044 - acc: 0.9983 - val_loss: 2.7066e-07 - val_acc: 1.0000\n",
            "Epoch 286/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0040 - acc: 0.9990 - val_loss: 3.3993e-07 - val_acc: 1.0000\n",
            "Epoch 287/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0053 - acc: 0.9989 - val_loss: 2.6846e-07 - val_acc: 1.0000\n",
            "Epoch 288/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 2.4670e-07 - val_acc: 1.0000\n",
            "Epoch 289/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0039 - acc: 0.9989 - val_loss: 2.3055e-07 - val_acc: 1.0000\n",
            "Epoch 290/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0059 - acc: 0.9984 - val_loss: 3.1507e-07 - val_acc: 1.0000\n",
            "Epoch 291/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0038 - acc: 0.9986 - val_loss: 2.5386e-07 - val_acc: 1.0000\n",
            "Epoch 292/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0052 - acc: 0.9988 - val_loss: 2.9582e-07 - val_acc: 1.0000\n",
            "Epoch 293/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0070 - acc: 0.9979 - val_loss: 2.5988e-07 - val_acc: 1.0000\n",
            "Epoch 294/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0023 - acc: 0.9994 - val_loss: 2.8551e-07 - val_acc: 1.0000\n",
            "Epoch 295/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 2.8324e-07 - val_acc: 1.0000\n",
            "Epoch 296/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0040 - acc: 0.9983 - val_loss: 2.8318e-07 - val_acc: 1.0000\n",
            "Epoch 297/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0054 - acc: 0.9990 - val_loss: 2.9677e-07 - val_acc: 1.0000\n",
            "Epoch 298/450\n",
            "10000/10000 [==============================] - 2s 213us/step - loss: 0.0056 - acc: 0.9985 - val_loss: 2.1791e-07 - val_acc: 1.0000\n",
            "Epoch 299/450\n",
            "10000/10000 [==============================] - 2s 214us/step - loss: 0.0093 - acc: 0.9980 - val_loss: 2.1070e-07 - val_acc: 1.0000\n",
            "Epoch 300/450\n",
            "10000/10000 [==============================] - 2s 203us/step - loss: 0.0058 - acc: 0.9986 - val_loss: 2.2376e-07 - val_acc: 1.0000\n",
            "Epoch 301/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0065 - acc: 0.9980 - val_loss: 2.2018e-07 - val_acc: 1.0000\n",
            "Epoch 302/450\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 2.1869e-07 - val_acc: 1.0000\n",
            "Epoch 303/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0040 - acc: 0.9987 - val_loss: 2.0075e-07 - val_acc: 1.0000\n",
            "Epoch 304/450\n",
            "10000/10000 [==============================] - 2s 201us/step - loss: 0.0078 - acc: 0.9973 - val_loss: 1.9532e-07 - val_acc: 1.0000\n",
            "Epoch 305/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0073 - acc: 0.9980 - val_loss: 2.6578e-07 - val_acc: 1.0000\n",
            "Epoch 306/450\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.0063 - acc: 0.9976 - val_loss: 2.0623e-07 - val_acc: 1.0000\n",
            "Epoch 307/450\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.0045 - acc: 0.9986 - val_loss: 2.1821e-07 - val_acc: 1.0000\n",
            "Epoch 308/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0054 - acc: 0.9987 - val_loss: 2.1303e-07 - val_acc: 1.0000\n",
            "Epoch 309/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0043 - acc: 0.9985 - val_loss: 1.9795e-07 - val_acc: 1.0000\n",
            "Epoch 310/450\n",
            "10000/10000 [==============================] - 2s 207us/step - loss: 0.0051 - acc: 0.9983 - val_loss: 2.3085e-07 - val_acc: 1.0000\n",
            "Epoch 311/450\n",
            "10000/10000 [==============================] - 2s 203us/step - loss: 0.0040 - acc: 0.9985 - val_loss: 1.9485e-07 - val_acc: 1.0000\n",
            "Epoch 312/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0045 - acc: 0.9987 - val_loss: 1.9437e-07 - val_acc: 1.0000\n",
            "Epoch 313/450\n",
            "10000/10000 [==============================] - 2s 203us/step - loss: 0.0049 - acc: 0.9985 - val_loss: 1.8024e-07 - val_acc: 1.0000\n",
            "Epoch 314/450\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.0074 - acc: 0.9981 - val_loss: 1.8805e-07 - val_acc: 1.0000\n",
            "Epoch 315/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0050 - acc: 0.9987 - val_loss: 1.8328e-07 - val_acc: 1.0000\n",
            "Epoch 316/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0046 - acc: 0.9988 - val_loss: 2.5249e-07 - val_acc: 1.0000\n",
            "Epoch 317/450\n",
            "10000/10000 [==============================] - 2s 206us/step - loss: 0.0051 - acc: 0.9986 - val_loss: 1.9944e-07 - val_acc: 1.0000\n",
            "Epoch 318/450\n",
            "10000/10000 [==============================] - 2s 204us/step - loss: 0.0050 - acc: 0.9987 - val_loss: 1.7118e-07 - val_acc: 1.0000\n",
            "Epoch 319/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0027 - acc: 0.9991 - val_loss: 1.7041e-07 - val_acc: 1.0000\n",
            "Epoch 320/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0087 - acc: 0.9984 - val_loss: 2.0081e-07 - val_acc: 1.0000\n",
            "Epoch 321/450\n",
            "10000/10000 [==============================] - 2s 204us/step - loss: 0.0028 - acc: 0.9988 - val_loss: 1.6242e-07 - val_acc: 1.0000\n",
            "Epoch 322/450\n",
            "10000/10000 [==============================] - 2s 205us/step - loss: 0.0047 - acc: 0.9987 - val_loss: 1.6290e-07 - val_acc: 1.0000\n",
            "Epoch 323/450\n",
            "10000/10000 [==============================] - 2s 203us/step - loss: 0.0040 - acc: 0.9984 - val_loss: 1.5914e-07 - val_acc: 1.0000\n",
            "Epoch 324/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 1.6922e-07 - val_acc: 1.0000\n",
            "Epoch 325/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0052 - acc: 0.9983 - val_loss: 1.8144e-07 - val_acc: 1.0000\n",
            "Epoch 326/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0042 - acc: 0.9984 - val_loss: 1.5634e-07 - val_acc: 1.0000\n",
            "Epoch 327/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0043 - acc: 0.9985 - val_loss: 1.6063e-07 - val_acc: 1.0000\n",
            "Epoch 328/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0060 - acc: 0.9985 - val_loss: 1.7583e-07 - val_acc: 1.0000\n",
            "Epoch 329/450\n",
            "10000/10000 [==============================] - 2s 206us/step - loss: 0.0046 - acc: 0.9989 - val_loss: 1.5330e-07 - val_acc: 1.0000\n",
            "Epoch 330/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0051 - acc: 0.9986 - val_loss: 1.5527e-07 - val_acc: 1.0000\n",
            "Epoch 331/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0028 - acc: 0.9991 - val_loss: 1.6123e-07 - val_acc: 1.0000\n",
            "Epoch 332/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 1.5104e-07 - val_acc: 1.0000\n",
            "Epoch 333/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0069 - acc: 0.9983 - val_loss: 1.6212e-07 - val_acc: 1.0000\n",
            "Epoch 334/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0042 - acc: 0.9989 - val_loss: 1.5098e-07 - val_acc: 1.0000\n",
            "Epoch 335/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5491e-07 - val_acc: 1.0000\n",
            "Epoch 336/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0037 - acc: 0.9991 - val_loss: 1.5396e-07 - val_acc: 1.0000\n",
            "Epoch 337/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 1.4472e-07 - val_acc: 1.0000\n",
            "Epoch 338/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0063 - acc: 0.9982 - val_loss: 1.8299e-07 - val_acc: 1.0000\n",
            "Epoch 339/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0059 - acc: 0.9977 - val_loss: 1.4275e-07 - val_acc: 1.0000\n",
            "Epoch 340/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0055 - acc: 0.9985 - val_loss: 1.4740e-07 - val_acc: 1.0000\n",
            "Epoch 341/450\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.0069 - acc: 0.9979 - val_loss: 1.5140e-07 - val_acc: 1.0000\n",
            "Epoch 342/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0035 - acc: 0.9987 - val_loss: 1.4377e-07 - val_acc: 1.0000\n",
            "Epoch 343/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 1.5169e-07 - val_acc: 1.0000\n",
            "Epoch 344/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0047 - acc: 0.9988 - val_loss: 1.3638e-07 - val_acc: 1.0000\n",
            "Epoch 345/450\n",
            "10000/10000 [==============================] - 2s 204us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 1.3578e-07 - val_acc: 1.0000\n",
            "Epoch 346/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0058 - acc: 0.9983 - val_loss: 1.4448e-07 - val_acc: 1.0000\n",
            "Epoch 347/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 1.3834e-07 - val_acc: 1.0000\n",
            "Epoch 348/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0063 - acc: 0.9982 - val_loss: 1.4728e-07 - val_acc: 1.0000\n",
            "Epoch 349/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 1.4490e-07 - val_acc: 1.0000\n",
            "Epoch 350/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0058 - acc: 0.9984 - val_loss: 1.4347e-07 - val_acc: 1.0000\n",
            "Epoch 351/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0031 - acc: 0.9989 - val_loss: 1.9789e-07 - val_acc: 1.0000\n",
            "Epoch 352/450\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.0038 - acc: 0.9985 - val_loss: 1.3494e-07 - val_acc: 1.0000\n",
            "Epoch 353/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0043 - acc: 0.9989 - val_loss: 1.5819e-07 - val_acc: 1.0000\n",
            "Epoch 354/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0057 - acc: 0.9984 - val_loss: 1.4126e-07 - val_acc: 1.0000\n",
            "Epoch 355/450\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.0047 - acc: 0.9984 - val_loss: 1.3435e-07 - val_acc: 1.0000\n",
            "Epoch 356/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0040 - acc: 0.9989 - val_loss: 1.3500e-07 - val_acc: 1.0000\n",
            "Epoch 357/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0067 - acc: 0.9988 - val_loss: 1.3596e-07 - val_acc: 1.0000\n",
            "Epoch 358/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0028 - acc: 0.9986 - val_loss: 1.3089e-07 - val_acc: 1.0000\n",
            "Epoch 359/450\n",
            "10000/10000 [==============================] - 2s 206us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 1.3363e-07 - val_acc: 1.0000\n",
            "Epoch 360/450\n",
            "10000/10000 [==============================] - 2s 207us/step - loss: 0.0057 - acc: 0.9985 - val_loss: 1.2851e-07 - val_acc: 1.0000\n",
            "Epoch 361/450\n",
            "10000/10000 [==============================] - 2s 204us/step - loss: 0.0040 - acc: 0.9987 - val_loss: 1.2881e-07 - val_acc: 1.0000\n",
            "Epoch 362/450\n",
            "10000/10000 [==============================] - 2s 211us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 1.2749e-07 - val_acc: 1.0000\n",
            "Epoch 363/450\n",
            "10000/10000 [==============================] - 2s 207us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 1.2958e-07 - val_acc: 1.0000\n",
            "Epoch 364/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 1.2976e-07 - val_acc: 1.0000\n",
            "Epoch 365/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.3226e-07 - val_acc: 1.0000\n",
            "Epoch 366/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 1.2356e-07 - val_acc: 1.0000\n",
            "Epoch 367/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0043 - acc: 0.9983 - val_loss: 1.2732e-07 - val_acc: 1.0000\n",
            "Epoch 368/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 1.2660e-07 - val_acc: 1.0000\n",
            "Epoch 369/450\n",
            "10000/10000 [==============================] - 2s 200us/step - loss: 0.0039 - acc: 0.9989 - val_loss: 1.3030e-07 - val_acc: 1.0000\n",
            "Epoch 370/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 1.2690e-07 - val_acc: 1.0000\n",
            "Epoch 371/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0065 - acc: 0.9977 - val_loss: 1.2445e-07 - val_acc: 1.0000\n",
            "Epoch 372/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0066 - acc: 0.9984 - val_loss: 1.2702e-07 - val_acc: 1.0000\n",
            "Epoch 373/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0031 - acc: 0.9990 - val_loss: 1.2624e-07 - val_acc: 1.0000\n",
            "Epoch 374/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0047 - acc: 0.9985 - val_loss: 1.2577e-07 - val_acc: 1.0000\n",
            "Epoch 375/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0031 - acc: 0.9987 - val_loss: 1.2511e-07 - val_acc: 1.0000\n",
            "Epoch 376/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 1.2487e-07 - val_acc: 1.0000\n",
            "Epoch 377/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 1.2714e-07 - val_acc: 1.0000\n",
            "Epoch 378/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0068 - acc: 0.9985 - val_loss: 1.2523e-07 - val_acc: 1.0000\n",
            "Epoch 379/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0051 - acc: 0.9986 - val_loss: 1.2481e-07 - val_acc: 1.0000\n",
            "Epoch 380/450\n",
            "10000/10000 [==============================] - 2s 198us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.2124e-07 - val_acc: 1.0000\n",
            "Epoch 381/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 1.2112e-07 - val_acc: 1.0000\n",
            "Epoch 382/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0041 - acc: 0.9990 - val_loss: 1.2851e-07 - val_acc: 1.0000\n",
            "Epoch 383/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 1.2326e-07 - val_acc: 1.0000\n",
            "Epoch 384/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0049 - acc: 0.9989 - val_loss: 1.2243e-07 - val_acc: 1.0000\n",
            "Epoch 385/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0040 - acc: 0.9985 - val_loss: 1.2344e-07 - val_acc: 1.0000\n",
            "Epoch 386/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0063 - acc: 0.9982 - val_loss: 1.2404e-07 - val_acc: 1.0000\n",
            "Epoch 387/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 1.2487e-07 - val_acc: 1.0000\n",
            "Epoch 388/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0059 - acc: 0.9984 - val_loss: 1.2684e-07 - val_acc: 1.0000\n",
            "Epoch 389/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0046 - acc: 0.9985 - val_loss: 1.2159e-07 - val_acc: 1.0000\n",
            "Epoch 390/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 1.2606e-07 - val_acc: 1.0000\n",
            "Epoch 391/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0054 - acc: 0.9985 - val_loss: 1.2606e-07 - val_acc: 1.0000\n",
            "Epoch 392/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0064 - acc: 0.9985 - val_loss: 1.3107e-07 - val_acc: 1.0000\n",
            "Epoch 393/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0039 - acc: 0.9987 - val_loss: 1.2422e-07 - val_acc: 1.0000\n",
            "Epoch 394/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0031 - acc: 0.9989 - val_loss: 1.2118e-07 - val_acc: 1.0000\n",
            "Epoch 395/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0055 - acc: 0.9990 - val_loss: 1.2386e-07 - val_acc: 1.0000\n",
            "Epoch 396/450\n",
            "10000/10000 [==============================] - 2s 186us/step - loss: 0.0046 - acc: 0.9987 - val_loss: 1.2064e-07 - val_acc: 1.0000\n",
            "Epoch 397/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0073 - acc: 0.9986 - val_loss: 1.2171e-07 - val_acc: 1.0000\n",
            "Epoch 398/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 1.2058e-07 - val_acc: 1.0000\n",
            "Epoch 399/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0055 - acc: 0.9981 - val_loss: 1.2016e-07 - val_acc: 1.0000\n",
            "Epoch 400/450\n",
            "10000/10000 [==============================] - 2s 184us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 1.1969e-07 - val_acc: 1.0000\n",
            "Epoch 401/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 1.2088e-07 - val_acc: 1.0000\n",
            "Epoch 402/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 1.1969e-07 - val_acc: 1.0000\n",
            "Epoch 403/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 1.2207e-07 - val_acc: 1.0000\n",
            "Epoch 404/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 1.1969e-07 - val_acc: 1.0000\n",
            "Epoch 405/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0043 - acc: 0.9986 - val_loss: 1.2106e-07 - val_acc: 1.0000\n",
            "Epoch 406/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0029 - acc: 0.9992 - val_loss: 1.2016e-07 - val_acc: 1.0000\n",
            "Epoch 407/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0031 - acc: 0.9989 - val_loss: 1.2016e-07 - val_acc: 1.0000\n",
            "Epoch 408/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 1.1945e-07 - val_acc: 1.0000\n",
            "Epoch 409/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0035 - acc: 0.9991 - val_loss: 1.2016e-07 - val_acc: 1.0000\n",
            "Epoch 410/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0055 - acc: 0.9991 - val_loss: 1.2130e-07 - val_acc: 1.0000\n",
            "Epoch 411/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 412/450\n",
            "10000/10000 [==============================] - 2s 199us/step - loss: 0.0053 - acc: 0.9990 - val_loss: 1.1969e-07 - val_acc: 1.0000\n",
            "Epoch 413/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 1.1945e-07 - val_acc: 1.0000\n",
            "Epoch 414/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 1.2094e-07 - val_acc: 1.0000\n",
            "Epoch 415/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0031 - acc: 0.9986 - val_loss: 1.2064e-07 - val_acc: 1.0000\n",
            "Epoch 416/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0019 - acc: 0.9993 - val_loss: 1.2207e-07 - val_acc: 1.0000\n",
            "Epoch 417/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0044 - acc: 0.9992 - val_loss: 1.1969e-07 - val_acc: 1.0000\n",
            "Epoch 418/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0050 - acc: 0.9989 - val_loss: 1.2118e-07 - val_acc: 1.0000\n",
            "Epoch 419/450\n",
            "10000/10000 [==============================] - 2s 192us/step - loss: 0.0041 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 420/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 1.2088e-07 - val_acc: 1.0000\n",
            "Epoch 421/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0043 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 422/450\n",
            "10000/10000 [==============================] - 2s 194us/step - loss: 0.0052 - acc: 0.9987 - val_loss: 1.2195e-07 - val_acc: 1.0000\n",
            "Epoch 423/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0024 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 424/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1945e-07 - val_acc: 1.0000\n",
            "Epoch 425/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 426/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0062 - acc: 0.9983 - val_loss: 1.1927e-07 - val_acc: 1.0000\n",
            "Epoch 427/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 1.2058e-07 - val_acc: 1.0000\n",
            "Epoch 428/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0049 - acc: 0.9987 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 429/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0046 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 430/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 431/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0044 - acc: 0.9986 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 432/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 1.2094e-07 - val_acc: 1.0000\n",
            "Epoch 433/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0060 - acc: 0.9981 - val_loss: 1.2702e-07 - val_acc: 1.0000\n",
            "Epoch 434/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0029 - acc: 0.9988 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 435/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0064 - acc: 0.9985 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 436/450\n",
            "10000/10000 [==============================] - 2s 196us/step - loss: 0.0041 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 437/450\n",
            "10000/10000 [==============================] - 2s 195us/step - loss: 0.0027 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 438/450\n",
            "10000/10000 [==============================] - 2s 189us/step - loss: 0.0063 - acc: 0.9983 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 439/450\n",
            "10000/10000 [==============================] - 2s 188us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 1.2404e-07 - val_acc: 1.0000\n",
            "Epoch 440/450\n",
            "10000/10000 [==============================] - 2s 185us/step - loss: 0.0024 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 441/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0037 - acc: 0.9985 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 442/450\n",
            "10000/10000 [==============================] - 2s 197us/step - loss: 0.0070 - acc: 0.9987 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 443/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0018 - acc: 0.9997 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 444/450\n",
            "10000/10000 [==============================] - 2s 202us/step - loss: 0.0043 - acc: 0.9986 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 445/450\n",
            "10000/10000 [==============================] - 2s 191us/step - loss: 0.0061 - acc: 0.9986 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 446/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0030 - acc: 0.9989 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 447/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0045 - acc: 0.9989 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 448/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0092 - acc: 0.9985 - val_loss: 1.1927e-07 - val_acc: 1.0000\n",
            "Epoch 449/450\n",
            "10000/10000 [==============================] - 2s 190us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 450/450\n",
            "10000/10000 [==============================] - 2s 193us/step - loss: 0.0052 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7_TcQKlXnaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'Z_chatbot_100_epochs.h5'\n",
        "model.save(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r31x_bGHXnaJ",
        "colab_type": "code",
        "outputId": "4fe5abec-a5f7-40e7-c790-ef10e767c1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        }
      },
      "source": [
        "#Lets plot the increase of accuracy as we increase the number of training epochs\n",
        "#We can see that without any training the acc is about 50%, random guessing\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAALJCAYAAACUZbS1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGgAA1iCoAQBgDYIaAADW8P8BGnalt2zes3IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfGRAIl-XnaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To load a model that we have already trained and saved:\n",
        "model.load_weights('Z_chatbot_100_epochs.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pThv7PJXnaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets check out the predictions on the test set:\n",
        "#These are just probabilities for every single word on the vocab\n",
        "pred_results = model.predict(([inputs_test,questions_test]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m_mb-uvXnaW",
        "colab_type": "code",
        "outputId": "891f09a6-d674-4b8a-b40b-27ea1a93858e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#First test data point\n",
        "test_data[9]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['desplazarse', 'a', 'san_francisco', '.'],\n",
              " ['cuando', 'llegar', '?'],\n",
              " 'no_se_date2')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGTkdB7HXnaZ",
        "colab_type": "code",
        "outputId": "fa102c76-a0a8-45a1-9fa2-f5155c0b2d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#These are the probabilities for the vocab words using the 1st sentence\n",
        "pred_results[9]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.9493665e-28, 4.7964866e-28, 6.7298355e-24, 4.8150762e-28,\n",
              "       5.1006748e-28, 1.0000000e+00, 5.9842700e-24, 4.9555632e-28,\n",
              "       4.9356033e-28, 5.0110713e-28, 5.0364445e-28, 4.9876713e-28,\n",
              "       5.0027436e-28, 4.3618030e-22, 1.1152727e-18, 5.1321175e-28,\n",
              "       4.8951020e-28, 2.3093934e-26, 5.0585489e-28, 4.9845710e-28,\n",
              "       4.9866250e-28, 1.0581593e-21, 4.8265331e-28, 4.9060196e-28,\n",
              "       5.0412117e-28, 9.3894097e-22, 3.0914984e-20, 2.1138338e-18,\n",
              "       4.9020907e-28, 4.9946778e-28, 1.7797318e-21, 5.0088734e-28,\n",
              "       5.0512597e-28, 4.9475166e-28, 1.7004157e-22, 4.9317259e-28,\n",
              "       4.9505938e-28, 4.8948781e-28, 4.9284162e-28, 5.0426927e-28,\n",
              "       4.8427995e-28, 4.9275515e-28, 4.8594915e-28, 4.9775024e-28,\n",
              "       4.7551316e-28, 1.2631157e-23, 7.7722514e-23, 5.0443663e-28,\n",
              "       4.8358950e-28, 4.9967169e-28, 4.8999033e-28, 5.1092432e-28,\n",
              "       1.3209066e-19, 2.2650955e-20, 4.9318391e-28, 5.0214430e-28,\n",
              "       4.7180007e-28, 5.1669850e-28, 2.6712340e-22, 5.0983208e-28,\n",
              "       4.9513304e-28, 2.2094133e-18, 2.6536053e-20, 5.0986516e-28,\n",
              "       4.9234555e-28, 5.0597839e-28, 2.8712240e-20, 2.3658114e-25,\n",
              "       5.0097145e-28, 9.6743465e-23, 4.7479720e-28, 5.0036218e-28,\n",
              "       2.2249990e-20, 4.9157609e-28, 4.9723024e-28, 9.3826024e-23,\n",
              "       5.0660032e-28, 4.8837804e-28, 5.0557712e-28], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohaUj7kIXnac",
        "colab_type": "code",
        "outputId": "0d2b8c32-6f09-416d-db33-b1f7a62436d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_max = np.argmax(pred_results[10])\n",
        "print(val_max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK7R8Wg_Xnap",
        "colab_type": "code",
        "outputId": "26e01992-9742-4e9e-c809-301b70e3d32a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "for key,val in tokenizer.word_index.items():\n",
        "    if val == val_max:\n",
        "        k = key\n",
        "print(k)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-b0469ff34f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mval_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_max' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUTxB56RXnav",
        "colab_type": "code",
        "outputId": "2152e970-88ec-46b5-d1e9-11e50d0e3349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#See probability:\n",
        "pred_results[10][val_max]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkYZk75hXnay",
        "colab_type": "code",
        "outputId": "21ea38b8-ea96-4f78-c932-cf9c577a67f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Now, we can make our own questions using the vocabulary we have\n",
        "vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.',\n",
              " '?',\n",
              " 'a',\n",
              " 'alquiler_coche',\n",
              " 'alvia',\n",
              " 'amsterdam',\n",
              " 'antes_del',\n",
              " 'autobus',\n",
              " 'ave',\n",
              " 'avion',\n",
              " 'barcelona',\n",
              " 'barco',\n",
              " 'coche',\n",
              " 'como',\n",
              " 'crucero',\n",
              " 'cuando',\n",
              " 'darlyn',\n",
              " 'date',\n",
              " 'date2',\n",
              " 'desearimos_llegar',\n",
              " 'desplazarse',\n",
              " 'desplzarnos',\n",
              " 'donde',\n",
              " 'eduard',\n",
              " 'el',\n",
              " 'el_cabo',\n",
              " 'el_regreso',\n",
              " 'el_viajaria',\n",
              " 'ellos',\n",
              " 'ellos_viajarian',\n",
              " 'en',\n",
              " 'ferdinand',\n",
              " 'ferry',\n",
              " 'filip',\n",
              " 'ir',\n",
              " 'john',\n",
              " 'la_intencion_es_viajar',\n",
              " 'la_llegada_seria',\n",
              " 'la_salida',\n",
              " 'linda',\n",
              " 'llegar',\n",
              " 'llegariamos',\n",
              " 'londres',\n",
              " 'margaret',\n",
              " 'mary',\n",
              " 'mediante',\n",
              " 'moscu',\n",
              " 'new_york',\n",
              " 'no',\n",
              " 'no_mas_tarde',\n",
              " 'no_se_como',\n",
              " 'no_se_date',\n",
              " 'no_se_date2',\n",
              " 'no_se_donde',\n",
              " 'nosotros',\n",
              " 'para',\n",
              " 'partiriamos',\n",
              " 'queremos_ir',\n",
              " 'queremos_salir',\n",
              " 'queremos_viajar',\n",
              " 'quiero_ir',\n",
              " 'saldriamos',\n",
              " 'salir',\n",
              " 'san_francisco',\n",
              " 'sebastian',\n",
              " 'seria',\n",
              " 'si',\n",
              " 'sobre',\n",
              " 'tenemos_que_llegar',\n",
              " 'tenemos_que_regresar',\n",
              " 'tren',\n",
              " 'venecia',\n",
              " 'viajamos',\n",
              " 'viajar',\n",
              " 'viajar_mediante',\n",
              " 'viajaria',\n",
              " 'viajarian',\n",
              " 'viaje_negocios',\n",
              " 'yo'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPy5d6ptXna7",
        "colab_type": "code",
        "outputId": "7d8d625f-9d14-4347-d25a-ddf84da5285e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "#ellos tenemos_que_llegar para date2 . / como ? / no_se_como\n",
        "#yo llegariamos el date2 . / cuando llegar ? / date2\n",
        "#ellos queriamos el date . / como ? / no_se_como\n",
        "#ir en coche . / donde ? / no_se_donde\n",
        "#john desplazarse a san_francisco . / cuando salir ? / no_se_date\n",
        "#yo llegariamos el date2 . / como ? / no_se_como\n",
        "#ferdinand quiero_ir a el_cabo . / cuando salir ? / no_se_date\n",
        "#ellos seria el date . / cuando llegar ? / no_se_date2\n",
        "#ferdinand quiero_ir a londres . / como ? / no_se_como\n",
        "#yo saldriamos el date . / como ? / no_se_como\n",
        "#yo tenemos_que_llegar no_mas_tarde date2 . / como ? / no_se_como\n",
        "#yo viaje_negocios a amsterdam . / como ? / no_se_como\n",
        "#filip desplazarse a el_cabo . / cuando salir ? / no_se_date\n",
        "#yo queriamos el date . / como ? / no_se_como\n",
        "#viajar_mediante en barco . / cuando llegar ? / no_se_date2\n",
        "#nosotros saldriamos el date . / cuando salir ? / date\n",
        "#viajar en alquiler_coche . / cuando salir ? / no_se_date\n",
        "#mary viaje_negocios a el_cabo . / cuando salir ? / no_se_date\n",
        "#nosotros tenemos_que_llegar sobre date2 . / cuando salir ? / no_se_date\n",
        "#my_story = ' ferdinand viajar a venecia . ellos queremos_viajar el date . yo tenemos_que_llegar el date2 . ' \n",
        "#my_story = ' ellos tenemos_que_llegar para date2 . ' \n",
        "my_story = ' nosotros queriamos ir  en  coche basura.'\n",
        "my_question = ' como  ?' \n",
        "#cuando salir ?\n",
        "#cuando llegar ?\n",
        "#my_question = ' Como ?'\n",
        "#my_question = \"Como quiere mary viajar ? \"\n",
        "#my_question = \"Cuando quiere mary llegar ? \"\n",
        "#my_question = ' Como quiere i viajar ?'\n",
        "#where is Anne ? / kitchen\n",
        "#my_question.split()\n",
        "#Put the data in the same format as before\n",
        "my_data = [(my_story.split(), my_question.split(),'no')]\n",
        "#Vectorize this data\n",
        "my_story, my_ques, my_ans = vectorize_stories(my_data)\n",
        "#Make the prediction\n",
        "pred_results = model.predict(([my_story,my_ques]))\n",
        "val_max = np.argmax(pred_results[0])\n",
        "#print(\"La matriz de resultado; \" ,pred_results[0])\n",
        "print(\"Val max es; \" ,val_max )\n",
        "#Correct prediction!\n",
        "for key,val in tokenizer.word_index.items():\n",
        "    if val == val_max:\n",
        "        k = key\n",
        "print(k)\n",
        "#Confidence\n",
        "pred_results[0][val_max]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "problema con nueva palabra queriamos\n",
            "problema con nueva palabra basura.\n",
            "Val max es;  17\n",
            "coche\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DS1N5uT1Bxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A T E N C I O N\n",
        "\n",
        "# entrena PARA FRASES COMPUESTAS\n",
        "\n",
        "\n",
        "#my_story = ' ir en coche a londres .'\n",
        "# donde ? ----->  RESPUESTA = londres\n",
        "# como ? ----->  RESPUESTA = no_se_como\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp3NkB5IiG45",
        "colab_type": "code",
        "outputId": "43e46c0f-8db7-4d67-e39a-c2144200e509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#https://nlpforhackers.io/complete-guide-to-spacy/\n",
        "!pip install -U spacy==2.0.18\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==2.0.18\n",
            "  Using cached https://files.pythonhosted.org/packages/ae/6e/a89da6b5c83f8811e46e3a9270c1aed90e9b9ee6c60faf52b7239e5d3d69/spacy-2.0.18-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (0.2.9)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2018.1.10)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2.21.0)\n",
            "Collecting thinc<6.13.0,>=6.12.1 (from spacy==2.0.18)\n",
            "  Using cached https://files.pythonhosted.org/packages/db/a7/46640a46fd707aeb204aa4257a70974b6a22a0204ba703164d803215776f/thinc-6.12.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (2019.6.16)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.10.0)\n",
            "\u001b[31mERROR: es-core-news-sm 2.1.0 has requirement spacy>=2.1.0, but you'll have spacy 2.0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.1.0 has requirement spacy>=2.1.0, but you'll have spacy 2.0.18 which is incompatible.\u001b[0m\n",
            "Installing collected packages: thinc, spacy\n",
            "  Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Found existing installation: spacy 2.1.8\n",
            "    Uninstalling spacy-2.1.8:\n",
            "      Successfully uninstalled spacy-2.1.8\n",
            "Successfully installed spacy-2.0.18 thinc-6.12.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K     |████████████████████████████████| 37.4MB 47.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.0.0-cp36-none-any.whl size=37405977 sha256=236f3f7713d0bf0bd0e6bfaedf969a91602cbd3d4afd79506ebf6262e92f5fb4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-81hh12hg/wheels/54/7c/d8/f86364af8fbba7258e14adae115f18dd2c91552406edc3fdaa\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.1.0\n",
            "    Uninstalling en-core-web-sm-2.1.0:\n",
            "      Successfully uninstalled en-core-web-sm-2.1.0\n",
            "Successfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_sm\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_sm')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fnCylG00mnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hli8MxvKXnbJ",
        "colab_type": "code",
        "outputId": "7e9d18bb-2f43-4bbc-9497-8ea9c6eadc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "#doc = nlp(\"These are apples. These are oranges.\")\n",
        "doc = nlp('ellos tenemos_que_llegar para date .  el_viajaria en avion . yo partiriamos el date .')\n",
        "#ellos tenemos_que_llegar para date2 . viajar_mediante en crucero . yo partiriamos el date .\n",
        "\n",
        "doc = nlp('el_regreso desearimos_llegar para date . la_salida viajaria el date . el_viajaria en tren . yo partiriamos el date . yo desplazarse a el_cabo . ')\n",
        "#margaret viajar a barcelona . / cuando salir ? / no_se_date\n",
        "\n",
        "#el_regreso desearimos_llegar para date .\n",
        "#sebastian desplazarse a barcelona .\n",
        "#la_salida viajaria el date .\n",
        "#yo desplazarse a el_cabo . \n",
        "#la_salida seria el date . \n",
        "\n",
        "my_questions =[' cuando llegar  ?','cuando salir ?','donde ?', ' Como ?']\n",
        "\n",
        "#my_question = ' cuando llegar  ?'\n",
        "#cuando salir ?\n",
        "#cuando llegar ?\n",
        "#my_question = ' Como ?'\n",
        "n = 0\n",
        "for sent in doc.sents:\n",
        "    #print(type(sent))\n",
        "    print(\"*\"*20)\n",
        "    my_story = sent.text\n",
        "    print(my_story)\n",
        "    print(n)\n",
        "    n +=1\n",
        "    print(\"*\"*20)\n",
        "    my_array_story = my_story.split()\n",
        "    print(my_array_story)\n",
        "    for my_question in my_questions:\n",
        "      print(my_question.split())\n",
        "      \n",
        "      my_data = [(my_array_story , my_question.split(),'no')]\n",
        "      #Vectorize this data\n",
        "      my_story, my_ques, my_ans = vectorize_stories(my_data)\n",
        "      #Make the prediction\n",
        "      pred_results = model.predict(([my_story,my_ques]))\n",
        "      val_max = np.argmax(pred_results[0])\n",
        "      #print(\"La matriz de resultado; \" ,pred_results[0])\n",
        "      #print(\"Val max es; \" ,val_max )\n",
        "      #Correct prediction!\n",
        "      for key,val in tokenizer.word_index.items():\n",
        "        if val == val_max:\n",
        "          k = key\n",
        "      print(\"Respuesta------->   \",k)\n",
        "      #pred_results[0][val_max]\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********************\n",
            "el_regreso desearimos_llegar para date .\n",
            "0\n",
            "********************\n",
            "['el_regreso', 'desearimos_llegar', 'para', 'date', '.']\n",
            "['cuando', 'llegar', '?']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-f4299be0bfae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mmy_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_array_story\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmy_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'no'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m#Vectorize this data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mmy_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_ques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_stories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0;31m#Make the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mpred_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_story\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_ques\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vectorize_stories' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yQU5n3owmLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = nlp(\"Cats are beautiful animals.\")\n",
        " \n",
        "doc1 = nlp(\"Dogs are awesome.\")\n",
        "doc2 = nlp(\"Some gorgeous creatures are felines.\")\n",
        "doc3 = nlp(\"Dolphins are swimming mammals.\")\n",
        " \n",
        "print(target.similarity(doc1))  # 0.8901765218466683\n",
        "print(target.similarity(doc2))  # 0.9115828449161616\n",
        "print(target.similarity(doc3))  # 0.7822956752876101"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZpLyG2NxNhl",
        "colab_type": "code",
        "outputId": "4d90749e-f318-4753-c571-3e01d5239ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import spacy\n",
        "from nltk import Tree\n",
        "#https://stackoverflow.com/questions/28618400/how-to-identify-the-subject-of-a-sentence\n",
        "\n",
        "\n",
        "\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"The fox and the cow jumps over the lazy dog.\")\n",
        "\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.orth_\n",
        "\n",
        "\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        jumps                  \n",
            "  ________|____________         \n",
            " |       fox          over     \n",
            " |    ____|____        |        \n",
            " |   |    |   cow     dog      \n",
            " |   |    |    |    ___|____    \n",
            " .  The  and  the the      lazy\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83cINuX-zl8P",
        "colab_type": "code",
        "outputId": "5d973f57-10b1-4f11-e163-e6e84b7d3afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "\n",
        "# M O D E L O S       D E      L E N G U A J E      S P A C Y\n",
        "\n",
        "#https://spacy.io/models/en\n",
        "\n",
        "\n",
        "#     I N G L E S\n",
        "\n",
        "\n",
        "!pip install -U spacy==2.0.18\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"The fox and the cow jumps over the lazy dog.\")\n",
        "\n",
        "\n",
        "\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.orth_\n",
        "\n",
        "\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
        "\n",
        "#     E S P A Ñ O L\n",
        "\n",
        "#!pip install -U spacy\n",
        "#!python -m spacy download es\n",
        "\n",
        "#!python -m spacy download es_core_news_sm\n",
        "#import es_core_news_sm\n",
        "#nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "#nlp = es_core_news_sm.load()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy==2.0.18 in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied, skipping upgrade: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2018.1.10)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (0.2.9)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (6.12.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (2019.6.16)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.10.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_sm\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_sm')\n",
            "\n",
            "        jumps                  \n",
            "  ________|____________         \n",
            " |       fox          over     \n",
            " |    ____|____        |        \n",
            " |   |    |   cow     dog      \n",
            " |   |    |    |    ___|____    \n",
            " .  The  and  the the      lazy\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUd0TpRNb9fd",
        "colab_type": "code",
        "outputId": "33bb74f1-9d91-4e5d-89de-efcbee522c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# M O D E L O S       D E      L E N G U A J E      S P A C Y\n",
        "\n",
        "#https://spacy.io/models/en\n",
        "\n",
        "\n",
        "\n",
        "#!pip install -U spacy==2.0.18\n",
        "\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#import spacy\n",
        "\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "#doc = nlp(\"The fox and the cow jumps over the lazy dog.\")\n",
        "\n",
        "\n",
        "\n",
        "#def to_nltk_tree(node):\n",
        "#    if node.n_lefts + node.n_rights > 0:\n",
        "#        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "#    else:\n",
        "#        return node.orth_\n",
        "\n",
        "\n",
        "#[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
        "\n",
        "\n",
        "#*****************************************************************\n",
        "\n",
        "#     E S P A Ñ O L\n",
        "\n",
        "#!pip install -U spacy\n",
        "!python -m spacy download es\n",
        "\n",
        "print(\" \")\n",
        "print(\"*\"*30)\n",
        "print(\" \")\n",
        "\n",
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "import spacy\n",
        "#import es_core_news_sm\n",
        "\n",
        "print(\" \")\n",
        "print(\"HOLA \")\n",
        "print(\" \")\n",
        "print(\"*\"*30)\n",
        "print(\" \")\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "doc = nlp(\"El coche corre desde abril del año pasado\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)\n",
        "#nlp = es_core_news_sm.load()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: es_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.0.0/es_core_news_sm-2.0.0.tar.gz#egg=es_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
            "\n",
            "    You can now load the model via spacy.load('es')\n",
            "\n",
            " \n",
            "******************************\n",
            " \n",
            "Requirement already satisfied: es_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.0.0/es_core_news_sm-2.0.0.tar.gz#egg=es_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/es_core_news_sm\n",
            "\n",
            "    You can now load the model via spacy.load('es_core_news_sm')\n",
            "\n",
            " \n",
            "HOLA \n",
            " \n",
            "******************************\n",
            " \n",
            "El DET det\n",
            "coche NOUN nsubj\n",
            "corre VERB ROOT\n",
            "desde ADP case\n",
            "abril NOUN obl\n",
            "del ADP case\n",
            "año NOUN compound\n",
            "pasado ADJ amod\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n75U2VW4sQ7",
        "colab_type": "code",
        "outputId": "a8b6aafd-1d5d-4192-b639-1cb3512a45ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "\n",
        "doc = nlp(\"nosotros queremos salir en Mayo a Venecia\")\n",
        "#Google esta en San Francisco\n",
        "#\"Apple is looking at buying U.K. on April 1st, 2018\"\n",
        "# filtrar fechas en Python\n",
        "#https://stackoverflow.com/questions/19994396/best-way-to-identify-and-extract-dates-from-text-python\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)\n",
        "\n",
        "print(\" \")\n",
        "print(\"*\"*30)\n",
        "print(\" \")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "print(\" \")\n",
        "print(\"*\"*30)\n",
        "print(\" \")\n",
        "\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.orth_\n",
        "\n",
        "\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nosotros PRON nsubj\n",
            "queremos VERB ROOT\n",
            "salir VERB xcomp\n",
            "en ADP case\n",
            "Mayo NOUN obl\n",
            "a ADP case\n",
            "Venecia PROPN obl\n",
            " \n",
            "******************************\n",
            " \n",
            "Mayo 27 31 LOC\n",
            "Venecia 34 41 LOC\n",
            " \n",
            "******************************\n",
            " \n",
            "         queremos              \n",
            "    ________|_______            \n",
            "   |              salir        \n",
            "   |         _______|______     \n",
            "   |       Mayo         Venecia\n",
            "   |        |              |    \n",
            "nosotros    en             a   \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAhIjNjnnViD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ESTE CODIGO ESTA EN :\n",
        "#SpaCy + Deep Learning for NLP- Creating a chatbot.ipynb\n",
        "\n",
        "def sentence_to_semi_sintax2(doc, doc2,doc3):  \n",
        "  #Y = []\n",
        "  iter_nsubj = 1\n",
        "  iter_pobj = 1\n",
        "  iter_dobj = 1\n",
        "  iter_attr = 1\n",
        "  dic_ele = {}\n",
        "  dic_inv_ele = {}\n",
        "\n",
        "  new_phase =\"\"\n",
        "  new_phase2 =\"\"\n",
        "  \n",
        "  last_last_token_type =\"\"\n",
        "  last_token_type =\"\"\n",
        "  last_token = \"\"\n",
        "  for token in doc:\n",
        "      #print(token.text, token.lemma_, token.pos_,  token.dep_ )\n",
        "      procesado = token.text\n",
        "      #if dic_inv_ele[token.text]:\n",
        "      if not token.text in dic_inv_ele:\n",
        "          #print(\"no existe \",token.text)\n",
        "      \n",
        "        if token.dep_ == \"nsubj\":\n",
        "          nsub = \"nsub\"+str(iter_nsubj)\n",
        "          dic_ele[nsub]=token.text\n",
        "          #print(token.text)\n",
        "          dic_inv_ele[token.text]=nsub\n",
        "          iter_nsubj +=1\n",
        "          procesado = nsub\n",
        "        elif token.dep_ == \"dobj\":\n",
        "          dobj = \"dobj\"+str(iter_dobj)\n",
        "          iter_dobj +=1\n",
        "          dic_ele[dobj]=token.text\n",
        "          dic_inv_ele[token.text]=dobj\n",
        "          procesado = dobj\n",
        "        elif token.dep_ == \"pobj\":\n",
        "          pobj = \"pobj\"+str(iter_dobj)\n",
        "          iter_dobj +=1\n",
        "          dic_inv_ele[token.text]=pobj\n",
        "          procesado = pobj\n",
        "        elif token.dep_ == \"attr\":\n",
        "          attr = \"attr\"+str(iter_attr)\n",
        "          iter_attr +=1\n",
        "          dic_ele[attr]=token.text\n",
        "          dic_inv_ele[token.text]=attr\n",
        "          procesado = attr\n",
        "        elif last_token == \"and\":\n",
        "          if last_last_token_type == \"nsubj\":\n",
        "            nsub = \"nsub\"+str(iter_nsubj)\n",
        "            iter_nsubj +=1\n",
        "            dic_ele[nsub]=token.text\n",
        "            dic_inv_ele[token.text]=nsub\n",
        "            procesado = nsub\n",
        "      else: # ya existte esa palabra\n",
        "        procesado = dic_inv_ele[token.text]\n",
        "        #print(\"existe \",token.text)\n",
        "      #new_phase = new_phase.join(token.text)\n",
        "      new_phase = new_phase + procesado + \" \"\n",
        "      last_last_token_type = last_token_type\n",
        "      last_token_type = token.dep_\n",
        "      last_token = token.text\n",
        " \n",
        "\n",
        "  last_last_token_type =\"\"\n",
        "  last_token_type =\"\"\n",
        "  last_token = \"\"\n",
        "  \n",
        "  for token2 in doc2:\n",
        "      #print(token.text, token.lemma_, token.pos_,  token.dep_ )\n",
        "      procesado2 = token2.text\n",
        "      \n",
        "      #if dic_inv_ele[token.text]:\n",
        "      if not token2.text in dic_inv_ele:\n",
        "          #print(\"no existe \",token2.text)\n",
        "      \n",
        "        if token2.dep_ == \"nsubj\":\n",
        "          nsub = \"nsub\"+str(iter_nsubj)\n",
        "          dic_ele[nsub]=token2.text\n",
        "          dic_inv_ele[token2.text]=nsub\n",
        "          iter_nsubj +=1\n",
        "          procesado2 = nsub\n",
        "        elif token2.dep_ == \"dobj\":\n",
        "          dobj = \"dobj\"+str(iter_dobj)\n",
        "          iter_dobj +=1\n",
        "          dic_ele[dobj]=token2.text\n",
        "          dic_inv_ele[token2.text]=dobj\n",
        "          procesado2 = dobj\n",
        "        elif token2.dep_ == \"pobj\":\n",
        "          pobj = \"pobj\"+str(iter_dobj)\n",
        "          iter_dobj +=1\n",
        "          dic_inv_ele[token2.text]=pobj\n",
        "          procesado2 = pobj\n",
        "        elif token2.dep_ == \"attr\":\n",
        "          attr = \"attr\"+str(iter_attr)\n",
        "          iter_attr +=1\n",
        "          dic_ele[attr]=token2.text\n",
        "          dic_inv_ele[token2.text]=attr\n",
        "          procesado2 = attr\n",
        "        elif last_token == \"and\":\n",
        "          if last_last_token_type == \"nsubj\":\n",
        "            nsub = \"nsub\"+str(iter_nsubj)\n",
        "            iter_nsubj +=1\n",
        "            dic_ele[nsub]=token2.text\n",
        "            dic_inv_ele[token2.text]=nsub\n",
        "            procesado2 = nsub\n",
        "      else: # ya existte esa palabra\n",
        "        procesado2 = dic_inv_ele[token2.text]\n",
        "        #print(\"existe \",token2.text)\n",
        "      #new_phase = new_phase.join(token2.text)\n",
        "      new_phase2 = new_phase2 + procesado2 + \" \"\n",
        "      last_last_token_type = last_token_type\n",
        "      last_token_type = token2.dep_\n",
        "      last_token = token2.text\n",
        "      \n",
        "  for token3 in doc3:\n",
        "      procesado3 = token3.text\n",
        "      \n",
        "      if not token3.text in dic_inv_ele:\n",
        "          #print(\"no existe \",token2.text)\n",
        "        procesado3 = token3.text\n",
        "        \n",
        "      else: # ya existte esa palabra\n",
        "        procesado3 = dic_inv_ele[token3.text]\n",
        "        \n",
        "  return(new_phase , new_phase2, procesado3 )\n",
        "\n",
        "\n",
        "doc = nlp(u'He and she buy a car. Mary is in the kitchen')\n",
        "doc_ = nlp(u'Where is Mary ?')\n",
        "doc3 = nlp(u'car')\n",
        "doc_ = nlp(u'He and she buy a car. Mary is in the kitchen ?')\n",
        "doc2 = nlp(u'Mary moved to the bathroom . Sandra journeyed to the bathroom . Mary went back to the bedroom . Daniel went back to the hallway . / Is Daniel in the bathroom ? / no')\n",
        "print(sentence_to_semi_sintax2(doc, doc_, doc3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR7dS2nVsUd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parsin DATE in python\n",
        "\n",
        "# https://stackoverflow.com/questions/19994396/best-way-to-identify-and-extract-dates-from-text-python\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Finding a verb with a subject from below — good\n",
        "\n",
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "doc = nlp(u\"Mary runs and walks sometimes\")\n",
        "\n",
        "# Finding a verb with a subject from below — good\n",
        "verbs = set()\n",
        "for possible_subject in doc:\n",
        "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
        "        verbs.add(possible_subject.head)\n",
        "print(verbs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}